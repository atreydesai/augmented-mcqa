
=== Run Set ===
Total configs: 60
By dataset type: {'arc_challenge': 15, 'arc_easy': 15, 'gpqa': 15, 'mmlu_pro': 15}
By distractor source: {'dhuman': 20, 'dmodel': 20, 'scratch': 20}

[1/60] opus_Nanbeige_Nanbeige4.1-3B_arc_challenge_dhuman_0H3M
  Shared client initialized for: Nanbeige/Nanbeige4.1-3B

=== Running Experiment: opus_Nanbeige_Nanbeige4.1-3B_arc_challenge_dhuman_0H3M ===
Model: Local (Nanbeige/Nanbeige4.1-3B)
Config: 0H+3M
Entries: 5
Start index: 0
Checkpoint save interval: 50
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:47:20 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:47:37 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:47:37 [model.py:1745] Using max model len 262144
INFO 02-20 00:47:40 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=0 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:47:41 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:47:41 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:47:41 [model.py:1745] Using max model len 262144
INFO 02-20 00:47:41 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=1 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:47:42 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:47:42 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:47:42 [model.py:1745] Using max model len 262144
INFO 02-20 00:47:42 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=2 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:47:43 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:47:44 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:47:44 [model.py:1745] Using max model len 262144
INFO 02-20 00:47:44 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=3 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:47:45 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:47:45 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:47:45 [model.py:1745] Using max model len 262144
INFO 02-20 00:47:45 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=4 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
üíæ Final checkpoint saved: /fs/nexus-projects/rlab/atrey/qgqa/augmented-mcqa/results/local_eval/local_eval_smoke_20260220_054548/smoke/smoke__Nanbeige_Nanbeige4_1-3B__opus__full/results/opus/Nanbeige_Nanbeige4.1-3B_arc_challenge_dhuman/0H3M/checkpoints/eval_checkpoint_5_20260220_054746.json

=== Results ===
Accuracy (successful entries): 0.00% | attempted=5 successful=0 failed=5
Behavioral: {'G': 0, 'H': 0, 'M': 0, '?': 0}
Entry failures logged: 5
Saved to: /fs/nexus-projects/rlab/atrey/qgqa/augmented-mcqa/results/local_eval/local_eval_smoke_20260220_054548/smoke/smoke__Nanbeige_Nanbeige4_1-3B__opus__full/results/opus/Nanbeige_Nanbeige4.1-3B_arc_challenge_dhuman/0H3M
  FAILED: error: zero successful entries

[2/60] opus_Nanbeige_Nanbeige4.1-3B_arc_challenge_dhuman_0H6M

=== Running Experiment: opus_Nanbeige_Nanbeige4.1-3B_arc_challenge_dhuman_0H6M ===
Model: Local (Nanbeige/Nanbeige4.1-3B)
Config: 0H+6M
Entries: 5
Start index: 0
Checkpoint save interval: 50
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:47:46 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:47:46 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:47:46 [model.py:1745] Using max model len 262144
INFO 02-20 00:47:46 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=0 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:47:47 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:47:47 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:47:47 [model.py:1745] Using max model len 262144
INFO 02-20 00:47:47 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=1 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:47:48 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:47:48 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:47:48 [model.py:1745] Using max model len 262144
INFO 02-20 00:47:48 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=2 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:47:49 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:47:49 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:47:49 [model.py:1745] Using max model len 262144
INFO 02-20 00:47:49 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=3 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:47:50 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:47:51 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:47:51 [model.py:1745] Using max model len 262144
INFO 02-20 00:47:51 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=4 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
üíæ Final checkpoint saved: /fs/nexus-projects/rlab/atrey/qgqa/augmented-mcqa/results/local_eval/local_eval_smoke_20260220_054548/smoke/smoke__Nanbeige_Nanbeige4_1-3B__opus__full/results/opus/Nanbeige_Nanbeige4.1-3B_arc_challenge_dhuman/0H6M/checkpoints/eval_checkpoint_5_20260220_054752.json

=== Results ===
Accuracy (successful entries): 0.00% | attempted=5 successful=0 failed=5
Behavioral: {'G': 0, 'H': 0, 'M': 0, '?': 0}
Entry failures logged: 5
Saved to: /fs/nexus-projects/rlab/atrey/qgqa/augmented-mcqa/results/local_eval/local_eval_smoke_20260220_054548/smoke/smoke__Nanbeige_Nanbeige4_1-3B__opus__full/results/opus/Nanbeige_Nanbeige4.1-3B_arc_challenge_dhuman/0H6M
  FAILED: error: zero successful entries

[3/60] opus_Nanbeige_Nanbeige4.1-3B_arc_challenge_dhuman_3H0M

=== Running Experiment: opus_Nanbeige_Nanbeige4.1-3B_arc_challenge_dhuman_3H0M ===
Model: Local (Nanbeige/Nanbeige4.1-3B)
Config: 3H+0M
Entries: 5
Start index: 0
Checkpoint save interval: 50
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:47:52 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:47:52 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:47:52 [model.py:1745] Using max model len 262144
INFO 02-20 00:47:52 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=0 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:47:53 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:47:53 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:47:53 [model.py:1745] Using max model len 262144
INFO 02-20 00:47:53 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=1 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:47:54 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:47:54 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:47:54 [model.py:1745] Using max model len 262144
INFO 02-20 00:47:54 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=2 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:47:55 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:47:55 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:47:55 [model.py:1745] Using max model len 262144
INFO 02-20 00:47:55 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=3 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:47:56 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:47:56 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:47:56 [model.py:1745] Using max model len 262144
INFO 02-20 00:47:56 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=4 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
üíæ Final checkpoint saved: /fs/nexus-projects/rlab/atrey/qgqa/augmented-mcqa/results/local_eval/local_eval_smoke_20260220_054548/smoke/smoke__Nanbeige_Nanbeige4_1-3B__opus__full/results/opus/Nanbeige_Nanbeige4.1-3B_arc_challenge_dhuman/3H0M/checkpoints/eval_checkpoint_5_20260220_054757.json

=== Results ===
Accuracy (successful entries): 0.00% | attempted=5 successful=0 failed=5
Behavioral: {'G': 0, 'H': 0, 'M': 0, '?': 0}
Entry failures logged: 5
Saved to: /fs/nexus-projects/rlab/atrey/qgqa/augmented-mcqa/results/local_eval/local_eval_smoke_20260220_054548/smoke/smoke__Nanbeige_Nanbeige4_1-3B__opus__full/results/opus/Nanbeige_Nanbeige4.1-3B_arc_challenge_dhuman/3H0M
  FAILED: error: zero successful entries

[4/60] opus_Nanbeige_Nanbeige4.1-3B_arc_challenge_dhuman_3H3M

=== Running Experiment: opus_Nanbeige_Nanbeige4.1-3B_arc_challenge_dhuman_3H3M ===
Model: Local (Nanbeige/Nanbeige4.1-3B)
Config: 3H+3M
Entries: 5
Start index: 0
Checkpoint save interval: 50
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:47:58 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:47:58 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:47:58 [model.py:1745] Using max model len 262144
INFO 02-20 00:47:58 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=0 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:47:59 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:47:59 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:47:59 [model.py:1745] Using max model len 262144
INFO 02-20 00:47:59 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=1 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:48:00 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:48:00 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:48:00 [model.py:1745] Using max model len 262144
INFO 02-20 00:48:00 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=2 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:48:01 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:48:01 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:48:01 [model.py:1745] Using max model len 262144
INFO 02-20 00:48:01 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=3 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:48:02 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:48:02 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:48:02 [model.py:1745] Using max model len 262144
INFO 02-20 00:48:02 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=4 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
üíæ Final checkpoint saved: /fs/nexus-projects/rlab/atrey/qgqa/augmented-mcqa/results/local_eval/local_eval_smoke_20260220_054548/smoke/smoke__Nanbeige_Nanbeige4_1-3B__opus__full/results/opus/Nanbeige_Nanbeige4.1-3B_arc_challenge_dhuman/3H3M/checkpoints/eval_checkpoint_5_20260220_054803.json

=== Results ===
Accuracy (successful entries): 0.00% | attempted=5 successful=0 failed=5
Behavioral: {'G': 0, 'H': 0, 'M': 0, '?': 0}
Entry failures logged: 5
Saved to: /fs/nexus-projects/rlab/atrey/qgqa/augmented-mcqa/results/local_eval/local_eval_smoke_20260220_054548/smoke/smoke__Nanbeige_Nanbeige4_1-3B__opus__full/results/opus/Nanbeige_Nanbeige4.1-3B_arc_challenge_dhuman/3H3M
  FAILED: error: zero successful entries

[5/60] opus_Nanbeige_Nanbeige4.1-3B_arc_challenge_dhuman_3H6M

=== Running Experiment: opus_Nanbeige_Nanbeige4.1-3B_arc_challenge_dhuman_3H6M ===
Model: Local (Nanbeige/Nanbeige4.1-3B)
Config: 3H+6M
Entries: 5
Start index: 0
Checkpoint save interval: 50
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:48:04 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:48:04 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:48:04 [model.py:1745] Using max model len 262144
INFO 02-20 00:48:04 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=0 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:48:05 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:48:05 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:48:05 [model.py:1745] Using max model len 262144
INFO 02-20 00:48:05 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=1 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:48:06 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:48:06 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:48:06 [model.py:1745] Using max model len 262144
INFO 02-20 00:48:06 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=2 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:48:07 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:48:07 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:48:07 [model.py:1745] Using max model len 262144
INFO 02-20 00:48:07 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=3 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:48:08 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:48:08 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:48:08 [model.py:1745] Using max model len 262144
INFO 02-20 00:48:08 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=4 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
üíæ Final checkpoint saved: /fs/nexus-projects/rlab/atrey/qgqa/augmented-mcqa/results/local_eval/local_eval_smoke_20260220_054548/smoke/smoke__Nanbeige_Nanbeige4_1-3B__opus__full/results/opus/Nanbeige_Nanbeige4.1-3B_arc_challenge_dhuman/3H6M/checkpoints/eval_checkpoint_5_20260220_054809.json

=== Results ===
Accuracy (successful entries): 0.00% | attempted=5 successful=0 failed=5
Behavioral: {'G': 0, 'H': 0, 'M': 0, '?': 0}
Entry failures logged: 5
Saved to: /fs/nexus-projects/rlab/atrey/qgqa/augmented-mcqa/results/local_eval/local_eval_smoke_20260220_054548/smoke/smoke__Nanbeige_Nanbeige4_1-3B__opus__full/results/opus/Nanbeige_Nanbeige4.1-3B_arc_challenge_dhuman/3H6M
  FAILED: error: zero successful entries

[6/60] opus_Nanbeige_Nanbeige4.1-3B_arc_challenge_dmodel_0H3M

=== Running Experiment: opus_Nanbeige_Nanbeige4.1-3B_arc_challenge_dmodel_0H3M ===
Model: Local (Nanbeige/Nanbeige4.1-3B)
Config: 0H+3M
Entries: 5
Start index: 0
Checkpoint save interval: 50
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:48:10 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:48:10 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:48:10 [model.py:1745] Using max model len 262144
INFO 02-20 00:48:10 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=0 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:48:11 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:48:11 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:48:11 [model.py:1745] Using max model len 262144
INFO 02-20 00:48:11 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=1 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:48:12 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:48:12 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:48:12 [model.py:1745] Using max model len 262144
INFO 02-20 00:48:12 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=2 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:48:13 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:48:13 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:48:13 [model.py:1745] Using max model len 262144
INFO 02-20 00:48:13 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=3 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:48:14 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:48:14 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:48:14 [model.py:1745] Using max model len 262144
INFO 02-20 00:48:14 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=4 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
üíæ Final checkpoint saved: /fs/nexus-projects/rlab/atrey/qgqa/augmented-mcqa/results/local_eval/local_eval_smoke_20260220_054548/smoke/smoke__Nanbeige_Nanbeige4_1-3B__opus__full/results/opus/Nanbeige_Nanbeige4.1-3B_arc_challenge_dmodel/0H3M/checkpoints/eval_checkpoint_5_20260220_054815.json

=== Results ===
Accuracy (successful entries): 0.00% | attempted=5 successful=0 failed=5
Behavioral: {'G': 0, 'H': 0, 'M': 0, '?': 0}
Entry failures logged: 5
Saved to: /fs/nexus-projects/rlab/atrey/qgqa/augmented-mcqa/results/local_eval/local_eval_smoke_20260220_054548/smoke/smoke__Nanbeige_Nanbeige4_1-3B__opus__full/results/opus/Nanbeige_Nanbeige4.1-3B_arc_challenge_dmodel/0H3M
  FAILED: error: zero successful entries

[7/60] opus_Nanbeige_Nanbeige4.1-3B_arc_challenge_dmodel_0H6M

=== Running Experiment: opus_Nanbeige_Nanbeige4.1-3B_arc_challenge_dmodel_0H6M ===
Model: Local (Nanbeige/Nanbeige4.1-3B)
Config: 0H+6M
Entries: 5
Start index: 0
Checkpoint save interval: 50
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:48:16 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:48:16 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:48:16 [model.py:1745] Using max model len 262144
INFO 02-20 00:48:16 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=0 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:48:17 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:48:17 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:48:17 [model.py:1745] Using max model len 262144
INFO 02-20 00:48:17 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=1 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:48:18 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:48:18 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:48:18 [model.py:1745] Using max model len 262144
INFO 02-20 00:48:18 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=2 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:48:19 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:48:19 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:48:19 [model.py:1745] Using max model len 262144
INFO 02-20 00:48:19 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=3 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:48:20 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:48:20 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:48:20 [model.py:1745] Using max model len 262144
INFO 02-20 00:48:20 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=4 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
üíæ Final checkpoint saved: /fs/nexus-projects/rlab/atrey/qgqa/augmented-mcqa/results/local_eval/local_eval_smoke_20260220_054548/smoke/smoke__Nanbeige_Nanbeige4_1-3B__opus__full/results/opus/Nanbeige_Nanbeige4.1-3B_arc_challenge_dmodel/0H6M/checkpoints/eval_checkpoint_5_20260220_054821.json

=== Results ===
Accuracy (successful entries): 0.00% | attempted=5 successful=0 failed=5
Behavioral: {'G': 0, 'H': 0, 'M': 0, '?': 0}
Entry failures logged: 5
Saved to: /fs/nexus-projects/rlab/atrey/qgqa/augmented-mcqa/results/local_eval/local_eval_smoke_20260220_054548/smoke/smoke__Nanbeige_Nanbeige4_1-3B__opus__full/results/opus/Nanbeige_Nanbeige4.1-3B_arc_challenge_dmodel/0H6M
  FAILED: error: zero successful entries

[8/60] opus_Nanbeige_Nanbeige4.1-3B_arc_challenge_dmodel_3H0M

=== Running Experiment: opus_Nanbeige_Nanbeige4.1-3B_arc_challenge_dmodel_3H0M ===
Model: Local (Nanbeige/Nanbeige4.1-3B)
Config: 3H+0M
Entries: 5
Start index: 0
Checkpoint save interval: 50
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:48:22 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:48:22 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:48:22 [model.py:1745] Using max model len 262144
INFO 02-20 00:48:22 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=0 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:48:23 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:48:23 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:48:23 [model.py:1745] Using max model len 262144
INFO 02-20 00:48:23 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=1 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:48:24 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:48:24 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:48:24 [model.py:1745] Using max model len 262144
INFO 02-20 00:48:24 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=2 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:48:25 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:48:25 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:48:25 [model.py:1745] Using max model len 262144
INFO 02-20 00:48:25 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=3 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:48:26 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:48:26 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:48:26 [model.py:1745] Using max model len 262144
INFO 02-20 00:48:26 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=4 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
üíæ Final checkpoint saved: /fs/nexus-projects/rlab/atrey/qgqa/augmented-mcqa/results/local_eval/local_eval_smoke_20260220_054548/smoke/smoke__Nanbeige_Nanbeige4_1-3B__opus__full/results/opus/Nanbeige_Nanbeige4.1-3B_arc_challenge_dmodel/3H0M/checkpoints/eval_checkpoint_5_20260220_054827.json

=== Results ===
Accuracy (successful entries): 0.00% | attempted=5 successful=0 failed=5
Behavioral: {'G': 0, 'H': 0, 'M': 0, '?': 0}
Entry failures logged: 5
Saved to: /fs/nexus-projects/rlab/atrey/qgqa/augmented-mcqa/results/local_eval/local_eval_smoke_20260220_054548/smoke/smoke__Nanbeige_Nanbeige4_1-3B__opus__full/results/opus/Nanbeige_Nanbeige4.1-3B_arc_challenge_dmodel/3H0M
  FAILED: error: zero successful entries

[9/60] opus_Nanbeige_Nanbeige4.1-3B_arc_challenge_dmodel_3H3M

=== Running Experiment: opus_Nanbeige_Nanbeige4.1-3B_arc_challenge_dmodel_3H3M ===
Model: Local (Nanbeige/Nanbeige4.1-3B)
Config: 3H+3M
Entries: 5
Start index: 0
Checkpoint save interval: 50
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:48:28 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:48:28 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:48:28 [model.py:1745] Using max model len 262144
INFO 02-20 00:48:28 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=0 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:48:29 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:48:29 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:48:29 [model.py:1745] Using max model len 262144
INFO 02-20 00:48:29 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=1 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:48:30 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:48:30 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:48:30 [model.py:1745] Using max model len 262144
INFO 02-20 00:48:30 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=2 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:48:31 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:48:31 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:48:31 [model.py:1745] Using max model len 262144
INFO 02-20 00:48:31 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=3 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:48:32 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:48:32 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:48:32 [model.py:1745] Using max model len 262144
INFO 02-20 00:48:32 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=4 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
üíæ Final checkpoint saved: /fs/nexus-projects/rlab/atrey/qgqa/augmented-mcqa/results/local_eval/local_eval_smoke_20260220_054548/smoke/smoke__Nanbeige_Nanbeige4_1-3B__opus__full/results/opus/Nanbeige_Nanbeige4.1-3B_arc_challenge_dmodel/3H3M/checkpoints/eval_checkpoint_5_20260220_054833.json

=== Results ===
Accuracy (successful entries): 0.00% | attempted=5 successful=0 failed=5
Behavioral: {'G': 0, 'H': 0, 'M': 0, '?': 0}
Entry failures logged: 5
Saved to: /fs/nexus-projects/rlab/atrey/qgqa/augmented-mcqa/results/local_eval/local_eval_smoke_20260220_054548/smoke/smoke__Nanbeige_Nanbeige4_1-3B__opus__full/results/opus/Nanbeige_Nanbeige4.1-3B_arc_challenge_dmodel/3H3M
  FAILED: error: zero successful entries

[10/60] opus_Nanbeige_Nanbeige4.1-3B_arc_challenge_dmodel_3H6M

=== Running Experiment: opus_Nanbeige_Nanbeige4.1-3B_arc_challenge_dmodel_3H6M ===
Model: Local (Nanbeige/Nanbeige4.1-3B)
Config: 3H+6M
Entries: 5
Start index: 0
Checkpoint save interval: 50
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:48:33 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:48:34 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:48:34 [model.py:1745] Using max model len 262144
INFO 02-20 00:48:34 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=0 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:48:35 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:48:35 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:48:35 [model.py:1745] Using max model len 262144
INFO 02-20 00:48:35 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=1 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:48:36 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:48:36 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:48:36 [model.py:1745] Using max model len 262144
INFO 02-20 00:48:36 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=2 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:48:37 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:48:37 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:48:37 [model.py:1745] Using max model len 262144
INFO 02-20 00:48:37 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=3 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:48:38 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:48:38 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:48:38 [model.py:1745] Using max model len 262144
INFO 02-20 00:48:38 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=4 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
üíæ Final checkpoint saved: /fs/nexus-projects/rlab/atrey/qgqa/augmented-mcqa/results/local_eval/local_eval_smoke_20260220_054548/smoke/smoke__Nanbeige_Nanbeige4_1-3B__opus__full/results/opus/Nanbeige_Nanbeige4.1-3B_arc_challenge_dmodel/3H6M/checkpoints/eval_checkpoint_5_20260220_054839.json

=== Results ===
Accuracy (successful entries): 0.00% | attempted=5 successful=0 failed=5
Behavioral: {'G': 0, 'H': 0, 'M': 0, '?': 0}
Entry failures logged: 5
Saved to: /fs/nexus-projects/rlab/atrey/qgqa/augmented-mcqa/results/local_eval/local_eval_smoke_20260220_054548/smoke/smoke__Nanbeige_Nanbeige4_1-3B__opus__full/results/opus/Nanbeige_Nanbeige4.1-3B_arc_challenge_dmodel/3H6M
  FAILED: error: zero successful entries

[11/60] opus_Nanbeige_Nanbeige4.1-3B_arc_challenge_scratch_0H3M

=== Running Experiment: opus_Nanbeige_Nanbeige4.1-3B_arc_challenge_scratch_0H3M ===
Model: Local (Nanbeige/Nanbeige4.1-3B)
Config: 0H+3M
Entries: 5
Start index: 0
Checkpoint save interval: 50
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:48:39 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:48:39 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:48:39 [model.py:1745] Using max model len 262144
INFO 02-20 00:48:39 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=0 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:48:40 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:48:41 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:48:41 [model.py:1745] Using max model len 262144
INFO 02-20 00:48:41 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=1 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:48:42 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:48:42 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:48:42 [model.py:1745] Using max model len 262144
INFO 02-20 00:48:42 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=2 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:48:43 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:48:43 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:48:43 [model.py:1745] Using max model len 262144
INFO 02-20 00:48:43 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=3 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:48:44 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:48:44 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:48:44 [model.py:1745] Using max model len 262144
INFO 02-20 00:48:44 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=4 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
üíæ Final checkpoint saved: /fs/nexus-projects/rlab/atrey/qgqa/augmented-mcqa/results/local_eval/local_eval_smoke_20260220_054548/smoke/smoke__Nanbeige_Nanbeige4_1-3B__opus__full/results/opus/Nanbeige_Nanbeige4.1-3B_arc_challenge_scratch/0H3M/checkpoints/eval_checkpoint_5_20260220_054845.json

=== Results ===
Accuracy (successful entries): 0.00% | attempted=5 successful=0 failed=5
Behavioral: {'G': 0, 'H': 0, 'M': 0, '?': 0}
Entry failures logged: 5
Saved to: /fs/nexus-projects/rlab/atrey/qgqa/augmented-mcqa/results/local_eval/local_eval_smoke_20260220_054548/smoke/smoke__Nanbeige_Nanbeige4_1-3B__opus__full/results/opus/Nanbeige_Nanbeige4.1-3B_arc_challenge_scratch/0H3M
  FAILED: error: zero successful entries

[12/60] opus_Nanbeige_Nanbeige4.1-3B_arc_challenge_scratch_0H6M

=== Running Experiment: opus_Nanbeige_Nanbeige4.1-3B_arc_challenge_scratch_0H6M ===
Model: Local (Nanbeige/Nanbeige4.1-3B)
Config: 0H+6M
Entries: 5
Start index: 0
Checkpoint save interval: 50
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:48:45 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:48:46 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:48:46 [model.py:1745] Using max model len 262144
INFO 02-20 00:48:46 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=0 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:48:47 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:48:47 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:48:47 [model.py:1745] Using max model len 262144
INFO 02-20 00:48:47 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=1 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:48:48 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:48:48 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:48:48 [model.py:1745] Using max model len 262144
INFO 02-20 00:48:48 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=2 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:48:49 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:48:49 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:48:49 [model.py:1745] Using max model len 262144
INFO 02-20 00:48:49 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=3 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:48:50 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:48:50 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:48:50 [model.py:1745] Using max model len 262144
INFO 02-20 00:48:50 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=4 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
üíæ Final checkpoint saved: /fs/nexus-projects/rlab/atrey/qgqa/augmented-mcqa/results/local_eval/local_eval_smoke_20260220_054548/smoke/smoke__Nanbeige_Nanbeige4_1-3B__opus__full/results/opus/Nanbeige_Nanbeige4.1-3B_arc_challenge_scratch/0H6M/checkpoints/eval_checkpoint_5_20260220_054851.json

=== Results ===
Accuracy (successful entries): 0.00% | attempted=5 successful=0 failed=5
Behavioral: {'G': 0, 'H': 0, 'M': 0, '?': 0}
Entry failures logged: 5
Saved to: /fs/nexus-projects/rlab/atrey/qgqa/augmented-mcqa/results/local_eval/local_eval_smoke_20260220_054548/smoke/smoke__Nanbeige_Nanbeige4_1-3B__opus__full/results/opus/Nanbeige_Nanbeige4.1-3B_arc_challenge_scratch/0H6M
  FAILED: error: zero successful entries

[13/60] opus_Nanbeige_Nanbeige4.1-3B_arc_challenge_scratch_3H0M

=== Running Experiment: opus_Nanbeige_Nanbeige4.1-3B_arc_challenge_scratch_3H0M ===
Model: Local (Nanbeige/Nanbeige4.1-3B)
Config: 3H+0M
Entries: 5
Start index: 0
Checkpoint save interval: 50
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:48:51 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:48:51 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:48:51 [model.py:1745] Using max model len 262144
INFO 02-20 00:48:51 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=0 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:48:52 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:48:53 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:48:53 [model.py:1745] Using max model len 262144
INFO 02-20 00:48:53 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=1 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:48:54 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:48:54 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:48:54 [model.py:1745] Using max model len 262144
INFO 02-20 00:48:54 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=2 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:48:55 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:48:55 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:48:55 [model.py:1745] Using max model len 262144
INFO 02-20 00:48:55 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=3 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:48:56 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:48:56 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:48:56 [model.py:1745] Using max model len 262144
INFO 02-20 00:48:56 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=4 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
üíæ Final checkpoint saved: /fs/nexus-projects/rlab/atrey/qgqa/augmented-mcqa/results/local_eval/local_eval_smoke_20260220_054548/smoke/smoke__Nanbeige_Nanbeige4_1-3B__opus__full/results/opus/Nanbeige_Nanbeige4.1-3B_arc_challenge_scratch/3H0M/checkpoints/eval_checkpoint_5_20260220_054857.json

=== Results ===
Accuracy (successful entries): 0.00% | attempted=5 successful=0 failed=5
Behavioral: {'G': 0, 'H': 0, 'M': 0, '?': 0}
Entry failures logged: 5
Saved to: /fs/nexus-projects/rlab/atrey/qgqa/augmented-mcqa/results/local_eval/local_eval_smoke_20260220_054548/smoke/smoke__Nanbeige_Nanbeige4_1-3B__opus__full/results/opus/Nanbeige_Nanbeige4.1-3B_arc_challenge_scratch/3H0M
  FAILED: error: zero successful entries

[14/60] opus_Nanbeige_Nanbeige4.1-3B_arc_challenge_scratch_3H3M

=== Running Experiment: opus_Nanbeige_Nanbeige4.1-3B_arc_challenge_scratch_3H3M ===
Model: Local (Nanbeige/Nanbeige4.1-3B)
Config: 3H+3M
Entries: 5
Start index: 0
Checkpoint save interval: 50
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:48:57 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:48:57 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:48:57 [model.py:1745] Using max model len 262144
INFO 02-20 00:48:57 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=0 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:48:58 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:48:58 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:48:58 [model.py:1745] Using max model len 262144
INFO 02-20 00:48:58 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=1 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:48:59 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:49:00 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:49:00 [model.py:1745] Using max model len 262144
INFO 02-20 00:49:00 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=2 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:49:01 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:49:01 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:49:01 [model.py:1745] Using max model len 262144
INFO 02-20 00:49:01 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=3 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:49:02 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:49:02 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:49:02 [model.py:1745] Using max model len 262144
INFO 02-20 00:49:02 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=4 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
üíæ Final checkpoint saved: /fs/nexus-projects/rlab/atrey/qgqa/augmented-mcqa/results/local_eval/local_eval_smoke_20260220_054548/smoke/smoke__Nanbeige_Nanbeige4_1-3B__opus__full/results/opus/Nanbeige_Nanbeige4.1-3B_arc_challenge_scratch/3H3M/checkpoints/eval_checkpoint_5_20260220_054903.json

=== Results ===
Accuracy (successful entries): 0.00% | attempted=5 successful=0 failed=5
Behavioral: {'G': 0, 'H': 0, 'M': 0, '?': 0}
Entry failures logged: 5
Saved to: /fs/nexus-projects/rlab/atrey/qgqa/augmented-mcqa/results/local_eval/local_eval_smoke_20260220_054548/smoke/smoke__Nanbeige_Nanbeige4_1-3B__opus__full/results/opus/Nanbeige_Nanbeige4.1-3B_arc_challenge_scratch/3H3M
  FAILED: error: zero successful entries

[15/60] opus_Nanbeige_Nanbeige4.1-3B_arc_challenge_scratch_3H6M

=== Running Experiment: opus_Nanbeige_Nanbeige4.1-3B_arc_challenge_scratch_3H6M ===
Model: Local (Nanbeige/Nanbeige4.1-3B)
Config: 3H+6M
Entries: 5
Start index: 0
Checkpoint save interval: 50
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:49:03 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:49:03 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:49:03 [model.py:1745] Using max model len 262144
INFO 02-20 00:49:03 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=0 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:49:04 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:49:04 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:49:04 [model.py:1745] Using max model len 262144
INFO 02-20 00:49:04 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=1 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:49:05 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:49:05 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:49:05 [model.py:1745] Using max model len 262144
INFO 02-20 00:49:05 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=2 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:49:06 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:49:07 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:49:07 [model.py:1745] Using max model len 262144
INFO 02-20 00:49:07 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=3 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:49:08 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:49:08 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:49:08 [model.py:1745] Using max model len 262144
INFO 02-20 00:49:08 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=4 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
üíæ Final checkpoint saved: /fs/nexus-projects/rlab/atrey/qgqa/augmented-mcqa/results/local_eval/local_eval_smoke_20260220_054548/smoke/smoke__Nanbeige_Nanbeige4_1-3B__opus__full/results/opus/Nanbeige_Nanbeige4.1-3B_arc_challenge_scratch/3H6M/checkpoints/eval_checkpoint_5_20260220_054909.json

=== Results ===
Accuracy (successful entries): 0.00% | attempted=5 successful=0 failed=5
Behavioral: {'G': 0, 'H': 0, 'M': 0, '?': 0}
Entry failures logged: 5
Saved to: /fs/nexus-projects/rlab/atrey/qgqa/augmented-mcqa/results/local_eval/local_eval_smoke_20260220_054548/smoke/smoke__Nanbeige_Nanbeige4_1-3B__opus__full/results/opus/Nanbeige_Nanbeige4.1-3B_arc_challenge_scratch/3H6M
  FAILED: error: zero successful entries

[16/60] opus_Nanbeige_Nanbeige4.1-3B_arc_easy_dhuman_0H3M

=== Running Experiment: opus_Nanbeige_Nanbeige4.1-3B_arc_easy_dhuman_0H3M ===
Model: Local (Nanbeige/Nanbeige4.1-3B)
Config: 0H+3M
Entries: 5
Start index: 0
Checkpoint save interval: 50
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:49:09 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:49:09 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:49:09 [model.py:1745] Using max model len 262144
INFO 02-20 00:49:09 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=0 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:49:10 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:49:10 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:49:10 [model.py:1745] Using max model len 262144
INFO 02-20 00:49:10 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=1 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:49:11 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:49:12 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:49:12 [model.py:1745] Using max model len 262144
INFO 02-20 00:49:12 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=2 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:49:13 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:49:13 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:49:13 [model.py:1745] Using max model len 262144
INFO 02-20 00:49:13 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=3 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:49:14 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:49:14 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:49:14 [model.py:1745] Using max model len 262144
INFO 02-20 00:49:14 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=4 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
üíæ Final checkpoint saved: /fs/nexus-projects/rlab/atrey/qgqa/augmented-mcqa/results/local_eval/local_eval_smoke_20260220_054548/smoke/smoke__Nanbeige_Nanbeige4_1-3B__opus__full/results/opus/Nanbeige_Nanbeige4.1-3B_arc_easy_dhuman/0H3M/checkpoints/eval_checkpoint_5_20260220_054915.json

=== Results ===
Accuracy (successful entries): 0.00% | attempted=5 successful=0 failed=5
Behavioral: {'G': 0, 'H': 0, 'M': 0, '?': 0}
Entry failures logged: 5
Saved to: /fs/nexus-projects/rlab/atrey/qgqa/augmented-mcqa/results/local_eval/local_eval_smoke_20260220_054548/smoke/smoke__Nanbeige_Nanbeige4_1-3B__opus__full/results/opus/Nanbeige_Nanbeige4.1-3B_arc_easy_dhuman/0H3M
  FAILED: error: zero successful entries

[17/60] opus_Nanbeige_Nanbeige4.1-3B_arc_easy_dhuman_0H6M

=== Running Experiment: opus_Nanbeige_Nanbeige4.1-3B_arc_easy_dhuman_0H6M ===
Model: Local (Nanbeige/Nanbeige4.1-3B)
Config: 0H+6M
Entries: 5
Start index: 0
Checkpoint save interval: 50
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:49:15 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:49:15 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:49:15 [model.py:1745] Using max model len 262144
INFO 02-20 00:49:15 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=0 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:49:16 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:49:16 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:49:16 [model.py:1745] Using max model len 262144
INFO 02-20 00:49:16 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=1 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:49:17 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:49:17 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:49:17 [model.py:1745] Using max model len 262144
INFO 02-20 00:49:17 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=2 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:49:18 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:49:19 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:49:19 [model.py:1745] Using max model len 262144
INFO 02-20 00:49:19 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=3 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:49:20 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:49:20 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:49:20 [model.py:1745] Using max model len 262144
INFO 02-20 00:49:20 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=4 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
üíæ Final checkpoint saved: /fs/nexus-projects/rlab/atrey/qgqa/augmented-mcqa/results/local_eval/local_eval_smoke_20260220_054548/smoke/smoke__Nanbeige_Nanbeige4_1-3B__opus__full/results/opus/Nanbeige_Nanbeige4.1-3B_arc_easy_dhuman/0H6M/checkpoints/eval_checkpoint_5_20260220_054921.json

=== Results ===
Accuracy (successful entries): 0.00% | attempted=5 successful=0 failed=5
Behavioral: {'G': 0, 'H': 0, 'M': 0, '?': 0}
Entry failures logged: 5
Saved to: /fs/nexus-projects/rlab/atrey/qgqa/augmented-mcqa/results/local_eval/local_eval_smoke_20260220_054548/smoke/smoke__Nanbeige_Nanbeige4_1-3B__opus__full/results/opus/Nanbeige_Nanbeige4.1-3B_arc_easy_dhuman/0H6M
  FAILED: error: zero successful entries

[18/60] opus_Nanbeige_Nanbeige4.1-3B_arc_easy_dhuman_3H0M

=== Running Experiment: opus_Nanbeige_Nanbeige4.1-3B_arc_easy_dhuman_3H0M ===
Model: Local (Nanbeige/Nanbeige4.1-3B)
Config: 3H+0M
Entries: 5
Start index: 0
Checkpoint save interval: 50
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:49:21 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:49:21 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:49:21 [model.py:1745] Using max model len 262144
INFO 02-20 00:49:21 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=0 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:49:22 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:49:22 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:49:22 [model.py:1745] Using max model len 262144
INFO 02-20 00:49:22 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=1 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:49:23 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:49:23 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:49:23 [model.py:1745] Using max model len 262144
INFO 02-20 00:49:23 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=2 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:49:24 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:49:25 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:49:25 [model.py:1745] Using max model len 262144
INFO 02-20 00:49:25 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=3 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:49:26 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:49:26 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:49:26 [model.py:1745] Using max model len 262144
INFO 02-20 00:49:26 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=4 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
üíæ Final checkpoint saved: /fs/nexus-projects/rlab/atrey/qgqa/augmented-mcqa/results/local_eval/local_eval_smoke_20260220_054548/smoke/smoke__Nanbeige_Nanbeige4_1-3B__opus__full/results/opus/Nanbeige_Nanbeige4.1-3B_arc_easy_dhuman/3H0M/checkpoints/eval_checkpoint_5_20260220_054927.json

=== Results ===
Accuracy (successful entries): 0.00% | attempted=5 successful=0 failed=5
Behavioral: {'G': 0, 'H': 0, 'M': 0, '?': 0}
Entry failures logged: 5
Saved to: /fs/nexus-projects/rlab/atrey/qgqa/augmented-mcqa/results/local_eval/local_eval_smoke_20260220_054548/smoke/smoke__Nanbeige_Nanbeige4_1-3B__opus__full/results/opus/Nanbeige_Nanbeige4.1-3B_arc_easy_dhuman/3H0M
  FAILED: error: zero successful entries

[19/60] opus_Nanbeige_Nanbeige4.1-3B_arc_easy_dhuman_3H3M

=== Running Experiment: opus_Nanbeige_Nanbeige4.1-3B_arc_easy_dhuman_3H3M ===
Model: Local (Nanbeige/Nanbeige4.1-3B)
Config: 3H+3M
Entries: 5
Start index: 0
Checkpoint save interval: 50
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:49:27 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:49:27 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:49:27 [model.py:1745] Using max model len 262144
INFO 02-20 00:49:27 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=0 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:49:28 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:49:28 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:49:28 [model.py:1745] Using max model len 262144
INFO 02-20 00:49:28 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=1 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:49:29 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:49:29 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:49:29 [model.py:1745] Using max model len 262144
INFO 02-20 00:49:29 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=2 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:49:30 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:49:30 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:49:30 [model.py:1745] Using max model len 262144
INFO 02-20 00:49:30 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=3 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:49:31 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:49:32 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:49:32 [model.py:1745] Using max model len 262144
INFO 02-20 00:49:32 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=4 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
üíæ Final checkpoint saved: /fs/nexus-projects/rlab/atrey/qgqa/augmented-mcqa/results/local_eval/local_eval_smoke_20260220_054548/smoke/smoke__Nanbeige_Nanbeige4_1-3B__opus__full/results/opus/Nanbeige_Nanbeige4.1-3B_arc_easy_dhuman/3H3M/checkpoints/eval_checkpoint_5_20260220_054933.json

=== Results ===
Accuracy (successful entries): 0.00% | attempted=5 successful=0 failed=5
Behavioral: {'G': 0, 'H': 0, 'M': 0, '?': 0}
Entry failures logged: 5
Saved to: /fs/nexus-projects/rlab/atrey/qgqa/augmented-mcqa/results/local_eval/local_eval_smoke_20260220_054548/smoke/smoke__Nanbeige_Nanbeige4_1-3B__opus__full/results/opus/Nanbeige_Nanbeige4.1-3B_arc_easy_dhuman/3H3M
  FAILED: error: zero successful entries

[20/60] opus_Nanbeige_Nanbeige4.1-3B_arc_easy_dhuman_3H6M

=== Running Experiment: opus_Nanbeige_Nanbeige4.1-3B_arc_easy_dhuman_3H6M ===
Model: Local (Nanbeige/Nanbeige4.1-3B)
Config: 3H+6M
Entries: 5
Start index: 0
Checkpoint save interval: 50
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:49:33 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:49:33 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:49:33 [model.py:1745] Using max model len 262144
INFO 02-20 00:49:33 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=0 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:49:34 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:49:34 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:49:34 [model.py:1745] Using max model len 262144
INFO 02-20 00:49:34 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=1 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:49:35 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:49:35 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:49:35 [model.py:1745] Using max model len 262144
INFO 02-20 00:49:35 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=2 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:49:36 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:49:36 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:49:36 [model.py:1745] Using max model len 262144
INFO 02-20 00:49:36 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=3 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:49:37 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:49:37 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:49:37 [model.py:1745] Using max model len 262144
INFO 02-20 00:49:37 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=4 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
üíæ Final checkpoint saved: /fs/nexus-projects/rlab/atrey/qgqa/augmented-mcqa/results/local_eval/local_eval_smoke_20260220_054548/smoke/smoke__Nanbeige_Nanbeige4_1-3B__opus__full/results/opus/Nanbeige_Nanbeige4.1-3B_arc_easy_dhuman/3H6M/checkpoints/eval_checkpoint_5_20260220_054938.json

=== Results ===
Accuracy (successful entries): 0.00% | attempted=5 successful=0 failed=5
Behavioral: {'G': 0, 'H': 0, 'M': 0, '?': 0}
Entry failures logged: 5
Saved to: /fs/nexus-projects/rlab/atrey/qgqa/augmented-mcqa/results/local_eval/local_eval_smoke_20260220_054548/smoke/smoke__Nanbeige_Nanbeige4_1-3B__opus__full/results/opus/Nanbeige_Nanbeige4.1-3B_arc_easy_dhuman/3H6M
  FAILED: error: zero successful entries

[21/60] opus_Nanbeige_Nanbeige4.1-3B_arc_easy_dmodel_0H3M

=== Running Experiment: opus_Nanbeige_Nanbeige4.1-3B_arc_easy_dmodel_0H3M ===
Model: Local (Nanbeige/Nanbeige4.1-3B)
Config: 0H+3M
Entries: 5
Start index: 0
Checkpoint save interval: 50
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:49:39 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:49:39 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:49:39 [model.py:1745] Using max model len 262144
INFO 02-20 00:49:39 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=0 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:49:40 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:49:40 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:49:40 [model.py:1745] Using max model len 262144
INFO 02-20 00:49:40 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=1 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:49:41 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:49:41 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:49:41 [model.py:1745] Using max model len 262144
INFO 02-20 00:49:41 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=2 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:49:42 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:49:42 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:49:42 [model.py:1745] Using max model len 262144
INFO 02-20 00:49:42 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=3 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:49:43 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:49:43 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:49:43 [model.py:1745] Using max model len 262144
INFO 02-20 00:49:43 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=4 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
üíæ Final checkpoint saved: /fs/nexus-projects/rlab/atrey/qgqa/augmented-mcqa/results/local_eval/local_eval_smoke_20260220_054548/smoke/smoke__Nanbeige_Nanbeige4_1-3B__opus__full/results/opus/Nanbeige_Nanbeige4.1-3B_arc_easy_dmodel/0H3M/checkpoints/eval_checkpoint_5_20260220_054944.json

=== Results ===
Accuracy (successful entries): 0.00% | attempted=5 successful=0 failed=5
Behavioral: {'G': 0, 'H': 0, 'M': 0, '?': 0}
Entry failures logged: 5
Saved to: /fs/nexus-projects/rlab/atrey/qgqa/augmented-mcqa/results/local_eval/local_eval_smoke_20260220_054548/smoke/smoke__Nanbeige_Nanbeige4_1-3B__opus__full/results/opus/Nanbeige_Nanbeige4.1-3B_arc_easy_dmodel/0H3M
  FAILED: error: zero successful entries

[22/60] opus_Nanbeige_Nanbeige4.1-3B_arc_easy_dmodel_0H6M

=== Running Experiment: opus_Nanbeige_Nanbeige4.1-3B_arc_easy_dmodel_0H6M ===
Model: Local (Nanbeige/Nanbeige4.1-3B)
Config: 0H+6M
Entries: 5
Start index: 0
Checkpoint save interval: 50
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:49:45 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:49:45 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:49:45 [model.py:1745] Using max model len 262144
INFO 02-20 00:49:45 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=0 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:49:46 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:49:46 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:49:46 [model.py:1745] Using max model len 262144
INFO 02-20 00:49:46 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=1 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:49:47 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:49:47 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:49:47 [model.py:1745] Using max model len 262144
INFO 02-20 00:49:47 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=2 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:49:48 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:49:48 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:49:48 [model.py:1745] Using max model len 262144
INFO 02-20 00:49:48 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=3 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:49:49 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:49:50 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:49:50 [model.py:1745] Using max model len 262144
INFO 02-20 00:49:50 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=4 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
üíæ Final checkpoint saved: /fs/nexus-projects/rlab/atrey/qgqa/augmented-mcqa/results/local_eval/local_eval_smoke_20260220_054548/smoke/smoke__Nanbeige_Nanbeige4_1-3B__opus__full/results/opus/Nanbeige_Nanbeige4.1-3B_arc_easy_dmodel/0H6M/checkpoints/eval_checkpoint_5_20260220_054951.json

=== Results ===
Accuracy (successful entries): 0.00% | attempted=5 successful=0 failed=5
Behavioral: {'G': 0, 'H': 0, 'M': 0, '?': 0}
Entry failures logged: 5
Saved to: /fs/nexus-projects/rlab/atrey/qgqa/augmented-mcqa/results/local_eval/local_eval_smoke_20260220_054548/smoke/smoke__Nanbeige_Nanbeige4_1-3B__opus__full/results/opus/Nanbeige_Nanbeige4.1-3B_arc_easy_dmodel/0H6M
  FAILED: error: zero successful entries

[23/60] opus_Nanbeige_Nanbeige4.1-3B_arc_easy_dmodel_3H0M

=== Running Experiment: opus_Nanbeige_Nanbeige4.1-3B_arc_easy_dmodel_3H0M ===
Model: Local (Nanbeige/Nanbeige4.1-3B)
Config: 3H+0M
Entries: 5
Start index: 0
Checkpoint save interval: 50
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:49:51 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:49:51 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:49:51 [model.py:1745] Using max model len 262144
INFO 02-20 00:49:51 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=0 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:49:52 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:49:52 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:49:52 [model.py:1745] Using max model len 262144
INFO 02-20 00:49:52 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=1 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:49:53 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:49:53 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:49:53 [model.py:1745] Using max model len 262144
INFO 02-20 00:49:53 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=2 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:49:54 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:49:54 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:49:54 [model.py:1745] Using max model len 262144
INFO 02-20 00:49:54 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=3 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:49:55 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:49:55 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:49:55 [model.py:1745] Using max model len 262144
INFO 02-20 00:49:55 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=4 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
üíæ Final checkpoint saved: /fs/nexus-projects/rlab/atrey/qgqa/augmented-mcqa/results/local_eval/local_eval_smoke_20260220_054548/smoke/smoke__Nanbeige_Nanbeige4_1-3B__opus__full/results/opus/Nanbeige_Nanbeige4.1-3B_arc_easy_dmodel/3H0M/checkpoints/eval_checkpoint_5_20260220_054956.json

=== Results ===
Accuracy (successful entries): 0.00% | attempted=5 successful=0 failed=5
Behavioral: {'G': 0, 'H': 0, 'M': 0, '?': 0}
Entry failures logged: 5
Saved to: /fs/nexus-projects/rlab/atrey/qgqa/augmented-mcqa/results/local_eval/local_eval_smoke_20260220_054548/smoke/smoke__Nanbeige_Nanbeige4_1-3B__opus__full/results/opus/Nanbeige_Nanbeige4.1-3B_arc_easy_dmodel/3H0M
  FAILED: error: zero successful entries

[24/60] opus_Nanbeige_Nanbeige4.1-3B_arc_easy_dmodel_3H3M

=== Running Experiment: opus_Nanbeige_Nanbeige4.1-3B_arc_easy_dmodel_3H3M ===
Model: Local (Nanbeige/Nanbeige4.1-3B)
Config: 3H+3M
Entries: 5
Start index: 0
Checkpoint save interval: 50
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:49:57 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:49:57 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:49:57 [model.py:1745] Using max model len 262144
INFO 02-20 00:49:57 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=0 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:49:58 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:49:58 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:49:58 [model.py:1745] Using max model len 262144
INFO 02-20 00:49:58 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=1 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:49:59 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:49:59 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:49:59 [model.py:1745] Using max model len 262144
INFO 02-20 00:49:59 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=2 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:50:00 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:50:00 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:50:00 [model.py:1745] Using max model len 262144
INFO 02-20 00:50:00 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=3 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:50:01 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:50:02 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:50:02 [model.py:1745] Using max model len 262144
INFO 02-20 00:50:02 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=4 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
üíæ Final checkpoint saved: /fs/nexus-projects/rlab/atrey/qgqa/augmented-mcqa/results/local_eval/local_eval_smoke_20260220_054548/smoke/smoke__Nanbeige_Nanbeige4_1-3B__opus__full/results/opus/Nanbeige_Nanbeige4.1-3B_arc_easy_dmodel/3H3M/checkpoints/eval_checkpoint_5_20260220_055003.json

=== Results ===
Accuracy (successful entries): 0.00% | attempted=5 successful=0 failed=5
Behavioral: {'G': 0, 'H': 0, 'M': 0, '?': 0}
Entry failures logged: 5
Saved to: /fs/nexus-projects/rlab/atrey/qgqa/augmented-mcqa/results/local_eval/local_eval_smoke_20260220_054548/smoke/smoke__Nanbeige_Nanbeige4_1-3B__opus__full/results/opus/Nanbeige_Nanbeige4.1-3B_arc_easy_dmodel/3H3M
  FAILED: error: zero successful entries

[25/60] opus_Nanbeige_Nanbeige4.1-3B_arc_easy_dmodel_3H6M

=== Running Experiment: opus_Nanbeige_Nanbeige4.1-3B_arc_easy_dmodel_3H6M ===
Model: Local (Nanbeige/Nanbeige4.1-3B)
Config: 3H+6M
Entries: 5
Start index: 0
Checkpoint save interval: 50
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:50:03 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:50:03 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:50:03 [model.py:1745] Using max model len 262144
INFO 02-20 00:50:03 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=0 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:50:04 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:50:04 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:50:04 [model.py:1745] Using max model len 262144
INFO 02-20 00:50:04 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=1 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:50:05 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:50:05 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:50:05 [model.py:1745] Using max model len 262144
INFO 02-20 00:50:05 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=2 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:50:06 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:50:06 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:50:06 [model.py:1745] Using max model len 262144
INFO 02-20 00:50:06 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=3 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:50:07 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:50:07 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:50:07 [model.py:1745] Using max model len 262144
INFO 02-20 00:50:07 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=4 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
üíæ Final checkpoint saved: /fs/nexus-projects/rlab/atrey/qgqa/augmented-mcqa/results/local_eval/local_eval_smoke_20260220_054548/smoke/smoke__Nanbeige_Nanbeige4_1-3B__opus__full/results/opus/Nanbeige_Nanbeige4.1-3B_arc_easy_dmodel/3H6M/checkpoints/eval_checkpoint_5_20260220_055009.json

=== Results ===
Accuracy (successful entries): 0.00% | attempted=5 successful=0 failed=5
Behavioral: {'G': 0, 'H': 0, 'M': 0, '?': 0}
Entry failures logged: 5
Saved to: /fs/nexus-projects/rlab/atrey/qgqa/augmented-mcqa/results/local_eval/local_eval_smoke_20260220_054548/smoke/smoke__Nanbeige_Nanbeige4_1-3B__opus__full/results/opus/Nanbeige_Nanbeige4.1-3B_arc_easy_dmodel/3H6M
  FAILED: error: zero successful entries

[26/60] opus_Nanbeige_Nanbeige4.1-3B_arc_easy_scratch_0H3M

=== Running Experiment: opus_Nanbeige_Nanbeige4.1-3B_arc_easy_scratch_0H3M ===
Model: Local (Nanbeige/Nanbeige4.1-3B)
Config: 0H+3M
Entries: 5
Start index: 0
Checkpoint save interval: 50
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:50:09 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:50:09 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:50:09 [model.py:1745] Using max model len 262144
INFO 02-20 00:50:09 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=0 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:50:10 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:50:10 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:50:10 [model.py:1745] Using max model len 262144
INFO 02-20 00:50:10 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=1 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:50:11 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:50:11 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:50:11 [model.py:1745] Using max model len 262144
INFO 02-20 00:50:11 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=2 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:50:12 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:50:13 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:50:13 [model.py:1745] Using max model len 262144
INFO 02-20 00:50:13 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=3 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:50:14 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:50:14 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:50:14 [model.py:1745] Using max model len 262144
INFO 02-20 00:50:14 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=4 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
üíæ Final checkpoint saved: /fs/nexus-projects/rlab/atrey/qgqa/augmented-mcqa/results/local_eval/local_eval_smoke_20260220_054548/smoke/smoke__Nanbeige_Nanbeige4_1-3B__opus__full/results/opus/Nanbeige_Nanbeige4.1-3B_arc_easy_scratch/0H3M/checkpoints/eval_checkpoint_5_20260220_055015.json

=== Results ===
Accuracy (successful entries): 0.00% | attempted=5 successful=0 failed=5
Behavioral: {'G': 0, 'H': 0, 'M': 0, '?': 0}
Entry failures logged: 5
Saved to: /fs/nexus-projects/rlab/atrey/qgqa/augmented-mcqa/results/local_eval/local_eval_smoke_20260220_054548/smoke/smoke__Nanbeige_Nanbeige4_1-3B__opus__full/results/opus/Nanbeige_Nanbeige4.1-3B_arc_easy_scratch/0H3M
  FAILED: error: zero successful entries

[27/60] opus_Nanbeige_Nanbeige4.1-3B_arc_easy_scratch_0H6M

=== Running Experiment: opus_Nanbeige_Nanbeige4.1-3B_arc_easy_scratch_0H6M ===
Model: Local (Nanbeige/Nanbeige4.1-3B)
Config: 0H+6M
Entries: 5
Start index: 0
Checkpoint save interval: 50
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:50:15 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:50:15 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:50:15 [model.py:1745] Using max model len 262144
INFO 02-20 00:50:15 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=0 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:50:16 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:50:16 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:50:16 [model.py:1745] Using max model len 262144
INFO 02-20 00:50:16 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=1 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:50:17 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:50:17 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:50:17 [model.py:1745] Using max model len 262144
INFO 02-20 00:50:17 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=2 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:50:18 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:50:19 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:50:19 [model.py:1745] Using max model len 262144
INFO 02-20 00:50:19 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=3 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:50:20 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:50:20 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:50:20 [model.py:1745] Using max model len 262144
INFO 02-20 00:50:20 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=4 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
üíæ Final checkpoint saved: /fs/nexus-projects/rlab/atrey/qgqa/augmented-mcqa/results/local_eval/local_eval_smoke_20260220_054548/smoke/smoke__Nanbeige_Nanbeige4_1-3B__opus__full/results/opus/Nanbeige_Nanbeige4.1-3B_arc_easy_scratch/0H6M/checkpoints/eval_checkpoint_5_20260220_055021.json

=== Results ===
Accuracy (successful entries): 0.00% | attempted=5 successful=0 failed=5
Behavioral: {'G': 0, 'H': 0, 'M': 0, '?': 0}
Entry failures logged: 5
Saved to: /fs/nexus-projects/rlab/atrey/qgqa/augmented-mcqa/results/local_eval/local_eval_smoke_20260220_054548/smoke/smoke__Nanbeige_Nanbeige4_1-3B__opus__full/results/opus/Nanbeige_Nanbeige4.1-3B_arc_easy_scratch/0H6M
  FAILED: error: zero successful entries

[28/60] opus_Nanbeige_Nanbeige4.1-3B_arc_easy_scratch_3H0M

=== Running Experiment: opus_Nanbeige_Nanbeige4.1-3B_arc_easy_scratch_3H0M ===
Model: Local (Nanbeige/Nanbeige4.1-3B)
Config: 3H+0M
Entries: 5
Start index: 0
Checkpoint save interval: 50
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:50:21 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:50:21 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:50:21 [model.py:1745] Using max model len 262144
INFO 02-20 00:50:21 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=0 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:50:22 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:50:22 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:50:22 [model.py:1745] Using max model len 262144
INFO 02-20 00:50:22 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=1 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:50:23 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:50:23 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:50:23 [model.py:1745] Using max model len 262144
INFO 02-20 00:50:23 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=2 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:50:24 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:50:25 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:50:25 [model.py:1745] Using max model len 262144
INFO 02-20 00:50:25 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=3 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:50:26 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:50:26 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:50:26 [model.py:1745] Using max model len 262144
INFO 02-20 00:50:26 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=4 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
üíæ Final checkpoint saved: /fs/nexus-projects/rlab/atrey/qgqa/augmented-mcqa/results/local_eval/local_eval_smoke_20260220_054548/smoke/smoke__Nanbeige_Nanbeige4_1-3B__opus__full/results/opus/Nanbeige_Nanbeige4.1-3B_arc_easy_scratch/3H0M/checkpoints/eval_checkpoint_5_20260220_055027.json

=== Results ===
Accuracy (successful entries): 0.00% | attempted=5 successful=0 failed=5
Behavioral: {'G': 0, 'H': 0, 'M': 0, '?': 0}
Entry failures logged: 5
Saved to: /fs/nexus-projects/rlab/atrey/qgqa/augmented-mcqa/results/local_eval/local_eval_smoke_20260220_054548/smoke/smoke__Nanbeige_Nanbeige4_1-3B__opus__full/results/opus/Nanbeige_Nanbeige4.1-3B_arc_easy_scratch/3H0M
  FAILED: error: zero successful entries

[29/60] opus_Nanbeige_Nanbeige4.1-3B_arc_easy_scratch_3H3M

=== Running Experiment: opus_Nanbeige_Nanbeige4.1-3B_arc_easy_scratch_3H3M ===
Model: Local (Nanbeige/Nanbeige4.1-3B)
Config: 3H+3M
Entries: 5
Start index: 0
Checkpoint save interval: 50
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:50:27 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:50:27 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:50:27 [model.py:1745] Using max model len 262144
INFO 02-20 00:50:27 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=0 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:50:28 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:50:28 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:50:28 [model.py:1745] Using max model len 262144
INFO 02-20 00:50:28 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=1 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:50:29 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:50:29 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:50:29 [model.py:1745] Using max model len 262144
INFO 02-20 00:50:29 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=2 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:50:30 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:50:31 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:50:31 [model.py:1745] Using max model len 262144
INFO 02-20 00:50:31 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=3 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:50:31 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:50:32 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:50:32 [model.py:1745] Using max model len 262144
INFO 02-20 00:50:32 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=4 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
üíæ Final checkpoint saved: /fs/nexus-projects/rlab/atrey/qgqa/augmented-mcqa/results/local_eval/local_eval_smoke_20260220_054548/smoke/smoke__Nanbeige_Nanbeige4_1-3B__opus__full/results/opus/Nanbeige_Nanbeige4.1-3B_arc_easy_scratch/3H3M/checkpoints/eval_checkpoint_5_20260220_055033.json

=== Results ===
Accuracy (successful entries): 0.00% | attempted=5 successful=0 failed=5
Behavioral: {'G': 0, 'H': 0, 'M': 0, '?': 0}
Entry failures logged: 5
Saved to: /fs/nexus-projects/rlab/atrey/qgqa/augmented-mcqa/results/local_eval/local_eval_smoke_20260220_054548/smoke/smoke__Nanbeige_Nanbeige4_1-3B__opus__full/results/opus/Nanbeige_Nanbeige4.1-3B_arc_easy_scratch/3H3M
  FAILED: error: zero successful entries

[30/60] opus_Nanbeige_Nanbeige4.1-3B_arc_easy_scratch_3H6M

=== Running Experiment: opus_Nanbeige_Nanbeige4.1-3B_arc_easy_scratch_3H6M ===
Model: Local (Nanbeige/Nanbeige4.1-3B)
Config: 3H+6M
Entries: 5
Start index: 0
Checkpoint save interval: 50
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:50:33 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:50:33 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:50:33 [model.py:1745] Using max model len 262144
INFO 02-20 00:50:33 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=0 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:50:34 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:50:34 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:50:34 [model.py:1745] Using max model len 262144
INFO 02-20 00:50:34 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=1 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:50:35 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:50:35 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:50:35 [model.py:1745] Using max model len 262144
INFO 02-20 00:50:35 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=2 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:50:36 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:50:36 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:50:36 [model.py:1745] Using max model len 262144
INFO 02-20 00:50:36 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=3 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:50:37 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:50:37 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:50:37 [model.py:1745] Using max model len 262144
INFO 02-20 00:50:37 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=4 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
üíæ Final checkpoint saved: /fs/nexus-projects/rlab/atrey/qgqa/augmented-mcqa/results/local_eval/local_eval_smoke_20260220_054548/smoke/smoke__Nanbeige_Nanbeige4_1-3B__opus__full/results/opus/Nanbeige_Nanbeige4.1-3B_arc_easy_scratch/3H6M/checkpoints/eval_checkpoint_5_20260220_055039.json

=== Results ===
Accuracy (successful entries): 0.00% | attempted=5 successful=0 failed=5
Behavioral: {'G': 0, 'H': 0, 'M': 0, '?': 0}
Entry failures logged: 5
Saved to: /fs/nexus-projects/rlab/atrey/qgqa/augmented-mcqa/results/local_eval/local_eval_smoke_20260220_054548/smoke/smoke__Nanbeige_Nanbeige4_1-3B__opus__full/results/opus/Nanbeige_Nanbeige4.1-3B_arc_easy_scratch/3H6M
  FAILED: error: zero successful entries

[31/60] opus_Nanbeige_Nanbeige4.1-3B_gpqa_dhuman_0H3M

=== Running Experiment: opus_Nanbeige_Nanbeige4.1-3B_gpqa_dhuman_0H3M ===
Model: Local (Nanbeige/Nanbeige4.1-3B)
Config: 0H+3M
Entries: 5
Start index: 0
Checkpoint save interval: 50
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:50:39 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:50:39 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:50:39 [model.py:1745] Using max model len 262144
INFO 02-20 00:50:39 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=0 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:50:40 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:50:40 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:50:40 [model.py:1745] Using max model len 262144
INFO 02-20 00:50:40 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=1 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:50:41 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:50:41 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:50:41 [model.py:1745] Using max model len 262144
INFO 02-20 00:50:41 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=2 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:50:42 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:50:42 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:50:42 [model.py:1745] Using max model len 262144
INFO 02-20 00:50:42 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=3 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:50:43 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:50:44 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:50:44 [model.py:1745] Using max model len 262144
INFO 02-20 00:50:44 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=4 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
üíæ Final checkpoint saved: /fs/nexus-projects/rlab/atrey/qgqa/augmented-mcqa/results/local_eval/local_eval_smoke_20260220_054548/smoke/smoke__Nanbeige_Nanbeige4_1-3B__opus__full/results/opus/Nanbeige_Nanbeige4.1-3B_gpqa_dhuman/0H3M/checkpoints/eval_checkpoint_5_20260220_055045.json

=== Results ===
Accuracy (successful entries): 0.00% | attempted=5 successful=0 failed=5
Behavioral: {'G': 0, 'H': 0, 'M': 0, '?': 0}
Entry failures logged: 5
Saved to: /fs/nexus-projects/rlab/atrey/qgqa/augmented-mcqa/results/local_eval/local_eval_smoke_20260220_054548/smoke/smoke__Nanbeige_Nanbeige4_1-3B__opus__full/results/opus/Nanbeige_Nanbeige4.1-3B_gpqa_dhuman/0H3M
  FAILED: error: zero successful entries

[32/60] opus_Nanbeige_Nanbeige4.1-3B_gpqa_dhuman_0H6M

=== Running Experiment: opus_Nanbeige_Nanbeige4.1-3B_gpqa_dhuman_0H6M ===
Model: Local (Nanbeige/Nanbeige4.1-3B)
Config: 0H+6M
Entries: 5
Start index: 0
Checkpoint save interval: 50
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:50:45 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:50:45 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:50:45 [model.py:1745] Using max model len 262144
INFO 02-20 00:50:45 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=0 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:50:46 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:50:46 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:50:46 [model.py:1745] Using max model len 262144
INFO 02-20 00:50:46 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=1 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:50:47 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:50:47 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:50:47 [model.py:1745] Using max model len 262144
INFO 02-20 00:50:47 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=2 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:50:48 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:50:48 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:50:48 [model.py:1745] Using max model len 262144
INFO 02-20 00:50:48 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=3 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:50:49 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:50:49 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:50:49 [model.py:1745] Using max model len 262144
INFO 02-20 00:50:49 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=4 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
üíæ Final checkpoint saved: /fs/nexus-projects/rlab/atrey/qgqa/augmented-mcqa/results/local_eval/local_eval_smoke_20260220_054548/smoke/smoke__Nanbeige_Nanbeige4_1-3B__opus__full/results/opus/Nanbeige_Nanbeige4.1-3B_gpqa_dhuman/0H6M/checkpoints/eval_checkpoint_5_20260220_055050.json

=== Results ===
Accuracy (successful entries): 0.00% | attempted=5 successful=0 failed=5
Behavioral: {'G': 0, 'H': 0, 'M': 0, '?': 0}
Entry failures logged: 5
Saved to: /fs/nexus-projects/rlab/atrey/qgqa/augmented-mcqa/results/local_eval/local_eval_smoke_20260220_054548/smoke/smoke__Nanbeige_Nanbeige4_1-3B__opus__full/results/opus/Nanbeige_Nanbeige4.1-3B_gpqa_dhuman/0H6M
  FAILED: error: zero successful entries

[33/60] opus_Nanbeige_Nanbeige4.1-3B_gpqa_dhuman_3H0M

=== Running Experiment: opus_Nanbeige_Nanbeige4.1-3B_gpqa_dhuman_3H0M ===
Model: Local (Nanbeige/Nanbeige4.1-3B)
Config: 3H+0M
Entries: 5
Start index: 0
Checkpoint save interval: 50
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:50:51 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:50:51 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:50:51 [model.py:1745] Using max model len 262144
INFO 02-20 00:50:51 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=0 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:50:52 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:50:52 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:50:52 [model.py:1745] Using max model len 262144
INFO 02-20 00:50:52 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=1 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:50:53 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:50:53 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:50:53 [model.py:1745] Using max model len 262144
INFO 02-20 00:50:53 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=2 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:50:54 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:50:54 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:50:54 [model.py:1745] Using max model len 262144
INFO 02-20 00:50:54 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=3 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:50:55 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:50:55 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:50:55 [model.py:1745] Using max model len 262144
INFO 02-20 00:50:55 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=4 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
üíæ Final checkpoint saved: /fs/nexus-projects/rlab/atrey/qgqa/augmented-mcqa/results/local_eval/local_eval_smoke_20260220_054548/smoke/smoke__Nanbeige_Nanbeige4_1-3B__opus__full/results/opus/Nanbeige_Nanbeige4.1-3B_gpqa_dhuman/3H0M/checkpoints/eval_checkpoint_5_20260220_055056.json

=== Results ===
Accuracy (successful entries): 0.00% | attempted=5 successful=0 failed=5
Behavioral: {'G': 0, 'H': 0, 'M': 0, '?': 0}
Entry failures logged: 5
Saved to: /fs/nexus-projects/rlab/atrey/qgqa/augmented-mcqa/results/local_eval/local_eval_smoke_20260220_054548/smoke/smoke__Nanbeige_Nanbeige4_1-3B__opus__full/results/opus/Nanbeige_Nanbeige4.1-3B_gpqa_dhuman/3H0M
  FAILED: error: zero successful entries

[34/60] opus_Nanbeige_Nanbeige4.1-3B_gpqa_dhuman_3H3M

=== Running Experiment: opus_Nanbeige_Nanbeige4.1-3B_gpqa_dhuman_3H3M ===
Model: Local (Nanbeige/Nanbeige4.1-3B)
Config: 3H+3M
Entries: 5
Start index: 0
Checkpoint save interval: 50
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:50:56 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:50:56 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:50:56 [model.py:1745] Using max model len 262144
INFO 02-20 00:50:56 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=0 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:50:57 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:50:58 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:50:58 [model.py:1745] Using max model len 262144
INFO 02-20 00:50:58 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=1 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:50:59 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:50:59 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:50:59 [model.py:1745] Using max model len 262144
INFO 02-20 00:50:59 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=2 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:51:00 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:51:00 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:51:00 [model.py:1745] Using max model len 262144
INFO 02-20 00:51:00 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=3 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:51:01 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:51:01 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:51:01 [model.py:1745] Using max model len 262144
INFO 02-20 00:51:01 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=4 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
üíæ Final checkpoint saved: /fs/nexus-projects/rlab/atrey/qgqa/augmented-mcqa/results/local_eval/local_eval_smoke_20260220_054548/smoke/smoke__Nanbeige_Nanbeige4_1-3B__opus__full/results/opus/Nanbeige_Nanbeige4.1-3B_gpqa_dhuman/3H3M/checkpoints/eval_checkpoint_5_20260220_055102.json

=== Results ===
Accuracy (successful entries): 0.00% | attempted=5 successful=0 failed=5
Behavioral: {'G': 0, 'H': 0, 'M': 0, '?': 0}
Entry failures logged: 5
Saved to: /fs/nexus-projects/rlab/atrey/qgqa/augmented-mcqa/results/local_eval/local_eval_smoke_20260220_054548/smoke/smoke__Nanbeige_Nanbeige4_1-3B__opus__full/results/opus/Nanbeige_Nanbeige4.1-3B_gpqa_dhuman/3H3M
  FAILED: error: zero successful entries

[35/60] opus_Nanbeige_Nanbeige4.1-3B_gpqa_dhuman_3H6M

=== Running Experiment: opus_Nanbeige_Nanbeige4.1-3B_gpqa_dhuman_3H6M ===
Model: Local (Nanbeige/Nanbeige4.1-3B)
Config: 3H+6M
Entries: 5
Start index: 0
Checkpoint save interval: 50
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:51:02 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:51:02 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:51:02 [model.py:1745] Using max model len 262144
INFO 02-20 00:51:02 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=0 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:51:03 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:51:03 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:51:03 [model.py:1745] Using max model len 262144
INFO 02-20 00:51:03 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=1 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:51:04 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:51:05 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:51:05 [model.py:1745] Using max model len 262144
INFO 02-20 00:51:05 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=2 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:51:06 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:51:06 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:51:06 [model.py:1745] Using max model len 262144
INFO 02-20 00:51:06 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=3 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:51:07 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:51:07 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:51:07 [model.py:1745] Using max model len 262144
INFO 02-20 00:51:07 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=4 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
üíæ Final checkpoint saved: /fs/nexus-projects/rlab/atrey/qgqa/augmented-mcqa/results/local_eval/local_eval_smoke_20260220_054548/smoke/smoke__Nanbeige_Nanbeige4_1-3B__opus__full/results/opus/Nanbeige_Nanbeige4.1-3B_gpqa_dhuman/3H6M/checkpoints/eval_checkpoint_5_20260220_055108.json

=== Results ===
Accuracy (successful entries): 0.00% | attempted=5 successful=0 failed=5
Behavioral: {'G': 0, 'H': 0, 'M': 0, '?': 0}
Entry failures logged: 5
Saved to: /fs/nexus-projects/rlab/atrey/qgqa/augmented-mcqa/results/local_eval/local_eval_smoke_20260220_054548/smoke/smoke__Nanbeige_Nanbeige4_1-3B__opus__full/results/opus/Nanbeige_Nanbeige4.1-3B_gpqa_dhuman/3H6M
  FAILED: error: zero successful entries

[36/60] opus_Nanbeige_Nanbeige4.1-3B_gpqa_dmodel_0H3M

=== Running Experiment: opus_Nanbeige_Nanbeige4.1-3B_gpqa_dmodel_0H3M ===
Model: Local (Nanbeige/Nanbeige4.1-3B)
Config: 0H+3M
Entries: 5
Start index: 0
Checkpoint save interval: 50
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:51:08 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:51:08 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:51:08 [model.py:1745] Using max model len 262144
INFO 02-20 00:51:08 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=0 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:51:09 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:51:09 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:51:09 [model.py:1745] Using max model len 262144
INFO 02-20 00:51:09 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=1 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:51:10 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:51:10 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:51:10 [model.py:1745] Using max model len 262144
INFO 02-20 00:51:10 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=2 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:51:11 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:51:12 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:51:12 [model.py:1745] Using max model len 262144
INFO 02-20 00:51:12 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=3 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:51:13 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:51:13 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:51:13 [model.py:1745] Using max model len 262144
INFO 02-20 00:51:13 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=4 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
üíæ Final checkpoint saved: /fs/nexus-projects/rlab/atrey/qgqa/augmented-mcqa/results/local_eval/local_eval_smoke_20260220_054548/smoke/smoke__Nanbeige_Nanbeige4_1-3B__opus__full/results/opus/Nanbeige_Nanbeige4.1-3B_gpqa_dmodel/0H3M/checkpoints/eval_checkpoint_5_20260220_055114.json

=== Results ===
Accuracy (successful entries): 0.00% | attempted=5 successful=0 failed=5
Behavioral: {'G': 0, 'H': 0, 'M': 0, '?': 0}
Entry failures logged: 5
Saved to: /fs/nexus-projects/rlab/atrey/qgqa/augmented-mcqa/results/local_eval/local_eval_smoke_20260220_054548/smoke/smoke__Nanbeige_Nanbeige4_1-3B__opus__full/results/opus/Nanbeige_Nanbeige4.1-3B_gpqa_dmodel/0H3M
  FAILED: error: zero successful entries

[37/60] opus_Nanbeige_Nanbeige4.1-3B_gpqa_dmodel_0H6M

=== Running Experiment: opus_Nanbeige_Nanbeige4.1-3B_gpqa_dmodel_0H6M ===
Model: Local (Nanbeige/Nanbeige4.1-3B)
Config: 0H+6M
Entries: 5
Start index: 0
Checkpoint save interval: 50
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:51:14 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:51:14 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:51:14 [model.py:1745] Using max model len 262144
INFO 02-20 00:51:14 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=0 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:51:15 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:51:15 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:51:15 [model.py:1745] Using max model len 262144
INFO 02-20 00:51:15 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=1 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:51:16 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:51:16 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:51:16 [model.py:1745] Using max model len 262144
INFO 02-20 00:51:16 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=2 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:51:17 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:51:17 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:51:17 [model.py:1745] Using max model len 262144
INFO 02-20 00:51:17 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=3 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:51:18 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:51:19 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:51:19 [model.py:1745] Using max model len 262144
INFO 02-20 00:51:19 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=4 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
üíæ Final checkpoint saved: /fs/nexus-projects/rlab/atrey/qgqa/augmented-mcqa/results/local_eval/local_eval_smoke_20260220_054548/smoke/smoke__Nanbeige_Nanbeige4_1-3B__opus__full/results/opus/Nanbeige_Nanbeige4.1-3B_gpqa_dmodel/0H6M/checkpoints/eval_checkpoint_5_20260220_055120.json

=== Results ===
Accuracy (successful entries): 0.00% | attempted=5 successful=0 failed=5
Behavioral: {'G': 0, 'H': 0, 'M': 0, '?': 0}
Entry failures logged: 5
Saved to: /fs/nexus-projects/rlab/atrey/qgqa/augmented-mcqa/results/local_eval/local_eval_smoke_20260220_054548/smoke/smoke__Nanbeige_Nanbeige4_1-3B__opus__full/results/opus/Nanbeige_Nanbeige4.1-3B_gpqa_dmodel/0H6M
  FAILED: error: zero successful entries

[38/60] opus_Nanbeige_Nanbeige4.1-3B_gpqa_dmodel_3H0M

=== Running Experiment: opus_Nanbeige_Nanbeige4.1-3B_gpqa_dmodel_3H0M ===
Model: Local (Nanbeige/Nanbeige4.1-3B)
Config: 3H+0M
Entries: 5
Start index: 0
Checkpoint save interval: 50
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:51:20 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:51:20 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:51:20 [model.py:1745] Using max model len 262144
INFO 02-20 00:51:20 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=0 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:51:21 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:51:21 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:51:21 [model.py:1745] Using max model len 262144
INFO 02-20 00:51:21 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=1 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:51:22 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:51:22 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:51:22 [model.py:1745] Using max model len 262144
INFO 02-20 00:51:22 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=2 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:51:23 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:51:23 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:51:23 [model.py:1745] Using max model len 262144
INFO 02-20 00:51:23 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=3 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:51:24 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:51:24 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:51:24 [model.py:1745] Using max model len 262144
INFO 02-20 00:51:24 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=4 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
üíæ Final checkpoint saved: /fs/nexus-projects/rlab/atrey/qgqa/augmented-mcqa/results/local_eval/local_eval_smoke_20260220_054548/smoke/smoke__Nanbeige_Nanbeige4_1-3B__opus__full/results/opus/Nanbeige_Nanbeige4.1-3B_gpqa_dmodel/3H0M/checkpoints/eval_checkpoint_5_20260220_055125.json

=== Results ===
Accuracy (successful entries): 0.00% | attempted=5 successful=0 failed=5
Behavioral: {'G': 0, 'H': 0, 'M': 0, '?': 0}
Entry failures logged: 5
Saved to: /fs/nexus-projects/rlab/atrey/qgqa/augmented-mcqa/results/local_eval/local_eval_smoke_20260220_054548/smoke/smoke__Nanbeige_Nanbeige4_1-3B__opus__full/results/opus/Nanbeige_Nanbeige4.1-3B_gpqa_dmodel/3H0M
  FAILED: error: zero successful entries

[39/60] opus_Nanbeige_Nanbeige4.1-3B_gpqa_dmodel_3H3M

=== Running Experiment: opus_Nanbeige_Nanbeige4.1-3B_gpqa_dmodel_3H3M ===
Model: Local (Nanbeige/Nanbeige4.1-3B)
Config: 3H+3M
Entries: 5
Start index: 0
Checkpoint save interval: 50
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:51:26 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:51:26 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:51:26 [model.py:1745] Using max model len 262144
INFO 02-20 00:51:26 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=0 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:51:27 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:51:27 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:51:27 [model.py:1745] Using max model len 262144
INFO 02-20 00:51:27 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=1 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:51:28 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:51:28 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:51:28 [model.py:1745] Using max model len 262144
INFO 02-20 00:51:28 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=2 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:51:29 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:51:29 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:51:29 [model.py:1745] Using max model len 262144
INFO 02-20 00:51:29 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=3 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:51:30 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:51:30 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:51:30 [model.py:1745] Using max model len 262144
INFO 02-20 00:51:30 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=4 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
üíæ Final checkpoint saved: /fs/nexus-projects/rlab/atrey/qgqa/augmented-mcqa/results/local_eval/local_eval_smoke_20260220_054548/smoke/smoke__Nanbeige_Nanbeige4_1-3B__opus__full/results/opus/Nanbeige_Nanbeige4.1-3B_gpqa_dmodel/3H3M/checkpoints/eval_checkpoint_5_20260220_055131.json

=== Results ===
Accuracy (successful entries): 0.00% | attempted=5 successful=0 failed=5
Behavioral: {'G': 0, 'H': 0, 'M': 0, '?': 0}
Entry failures logged: 5
Saved to: /fs/nexus-projects/rlab/atrey/qgqa/augmented-mcqa/results/local_eval/local_eval_smoke_20260220_054548/smoke/smoke__Nanbeige_Nanbeige4_1-3B__opus__full/results/opus/Nanbeige_Nanbeige4.1-3B_gpqa_dmodel/3H3M
  FAILED: error: zero successful entries

[40/60] opus_Nanbeige_Nanbeige4.1-3B_gpqa_dmodel_3H6M

=== Running Experiment: opus_Nanbeige_Nanbeige4.1-3B_gpqa_dmodel_3H6M ===
Model: Local (Nanbeige/Nanbeige4.1-3B)
Config: 3H+6M
Entries: 5
Start index: 0
Checkpoint save interval: 50
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:51:31 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:51:31 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:51:31 [model.py:1745] Using max model len 262144
INFO 02-20 00:51:31 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=0 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:51:33 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:51:33 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:51:33 [model.py:1745] Using max model len 262144
INFO 02-20 00:51:33 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=1 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:51:34 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:51:34 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:51:34 [model.py:1745] Using max model len 262144
INFO 02-20 00:51:34 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=2 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:51:35 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:51:35 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:51:35 [model.py:1745] Using max model len 262144
INFO 02-20 00:51:35 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=3 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:51:36 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:51:36 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:51:36 [model.py:1745] Using max model len 262144
INFO 02-20 00:51:36 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=4 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
üíæ Final checkpoint saved: /fs/nexus-projects/rlab/atrey/qgqa/augmented-mcqa/results/local_eval/local_eval_smoke_20260220_054548/smoke/smoke__Nanbeige_Nanbeige4_1-3B__opus__full/results/opus/Nanbeige_Nanbeige4.1-3B_gpqa_dmodel/3H6M/checkpoints/eval_checkpoint_5_20260220_055137.json

=== Results ===
Accuracy (successful entries): 0.00% | attempted=5 successful=0 failed=5
Behavioral: {'G': 0, 'H': 0, 'M': 0, '?': 0}
Entry failures logged: 5
Saved to: /fs/nexus-projects/rlab/atrey/qgqa/augmented-mcqa/results/local_eval/local_eval_smoke_20260220_054548/smoke/smoke__Nanbeige_Nanbeige4_1-3B__opus__full/results/opus/Nanbeige_Nanbeige4.1-3B_gpqa_dmodel/3H6M
  FAILED: error: zero successful entries

[41/60] opus_Nanbeige_Nanbeige4.1-3B_gpqa_scratch_0H3M

=== Running Experiment: opus_Nanbeige_Nanbeige4.1-3B_gpqa_scratch_0H3M ===
Model: Local (Nanbeige/Nanbeige4.1-3B)
Config: 0H+3M
Entries: 5
Start index: 0
Checkpoint save interval: 50
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:51:37 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:51:37 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:51:37 [model.py:1745] Using max model len 262144
INFO 02-20 00:51:37 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=0 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:51:38 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:51:38 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:51:38 [model.py:1745] Using max model len 262144
INFO 02-20 00:51:38 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=1 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:51:39 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:51:40 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:51:40 [model.py:1745] Using max model len 262144
INFO 02-20 00:51:40 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=2 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:51:41 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:51:41 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:51:41 [model.py:1745] Using max model len 262144
INFO 02-20 00:51:41 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=3 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:51:42 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:51:42 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:51:42 [model.py:1745] Using max model len 262144
INFO 02-20 00:51:42 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=4 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
üíæ Final checkpoint saved: /fs/nexus-projects/rlab/atrey/qgqa/augmented-mcqa/results/local_eval/local_eval_smoke_20260220_054548/smoke/smoke__Nanbeige_Nanbeige4_1-3B__opus__full/results/opus/Nanbeige_Nanbeige4.1-3B_gpqa_scratch/0H3M/checkpoints/eval_checkpoint_5_20260220_055143.json

=== Results ===
Accuracy (successful entries): 0.00% | attempted=5 successful=0 failed=5
Behavioral: {'G': 0, 'H': 0, 'M': 0, '?': 0}
Entry failures logged: 5
Saved to: /fs/nexus-projects/rlab/atrey/qgqa/augmented-mcqa/results/local_eval/local_eval_smoke_20260220_054548/smoke/smoke__Nanbeige_Nanbeige4_1-3B__opus__full/results/opus/Nanbeige_Nanbeige4.1-3B_gpqa_scratch/0H3M
  FAILED: error: zero successful entries

[42/60] opus_Nanbeige_Nanbeige4.1-3B_gpqa_scratch_0H6M

=== Running Experiment: opus_Nanbeige_Nanbeige4.1-3B_gpqa_scratch_0H6M ===
Model: Local (Nanbeige/Nanbeige4.1-3B)
Config: 0H+6M
Entries: 5
Start index: 0
Checkpoint save interval: 50
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:51:43 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:51:43 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:51:43 [model.py:1745] Using max model len 262144
INFO 02-20 00:51:43 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=0 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:51:44 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:51:44 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:51:44 [model.py:1745] Using max model len 262144
INFO 02-20 00:51:44 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=1 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:51:45 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:51:45 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:51:45 [model.py:1745] Using max model len 262144
INFO 02-20 00:51:45 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=2 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:51:46 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:51:46 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:51:46 [model.py:1745] Using max model len 262144
INFO 02-20 00:51:46 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=3 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:51:48 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:52:22 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:52:22 [model.py:1745] Using max model len 262144
INFO 02-20 00:52:22 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=4 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
üíæ Final checkpoint saved: /fs/nexus-projects/rlab/atrey/qgqa/augmented-mcqa/results/local_eval/local_eval_smoke_20260220_054548/smoke/smoke__Nanbeige_Nanbeige4_1-3B__opus__full/results/opus/Nanbeige_Nanbeige4.1-3B_gpqa_scratch/0H6M/checkpoints/eval_checkpoint_5_20260220_055223.json

=== Results ===
Accuracy (successful entries): 0.00% | attempted=5 successful=0 failed=5
Behavioral: {'G': 0, 'H': 0, 'M': 0, '?': 0}
Entry failures logged: 5
Saved to: /fs/nexus-projects/rlab/atrey/qgqa/augmented-mcqa/results/local_eval/local_eval_smoke_20260220_054548/smoke/smoke__Nanbeige_Nanbeige4_1-3B__opus__full/results/opus/Nanbeige_Nanbeige4.1-3B_gpqa_scratch/0H6M
  FAILED: error: zero successful entries

[43/60] opus_Nanbeige_Nanbeige4.1-3B_gpqa_scratch_3H0M

=== Running Experiment: opus_Nanbeige_Nanbeige4.1-3B_gpqa_scratch_3H0M ===
Model: Local (Nanbeige/Nanbeige4.1-3B)
Config: 3H+0M
Entries: 5
Start index: 0
Checkpoint save interval: 50
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:52:23 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:52:23 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:52:23 [model.py:1745] Using max model len 262144
INFO 02-20 00:52:23 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=0 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:52:24 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:52:24 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:52:24 [model.py:1745] Using max model len 262144
INFO 02-20 00:52:24 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=1 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:52:25 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:52:25 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:52:25 [model.py:1745] Using max model len 262144
INFO 02-20 00:52:25 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=2 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:52:26 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:52:26 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:52:26 [model.py:1745] Using max model len 262144
INFO 02-20 00:52:26 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=3 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:52:27 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:52:28 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:52:28 [model.py:1745] Using max model len 262144
INFO 02-20 00:52:28 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=4 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
üíæ Final checkpoint saved: /fs/nexus-projects/rlab/atrey/qgqa/augmented-mcqa/results/local_eval/local_eval_smoke_20260220_054548/smoke/smoke__Nanbeige_Nanbeige4_1-3B__opus__full/results/opus/Nanbeige_Nanbeige4.1-3B_gpqa_scratch/3H0M/checkpoints/eval_checkpoint_5_20260220_055229.json

=== Results ===
Accuracy (successful entries): 0.00% | attempted=5 successful=0 failed=5
Behavioral: {'G': 0, 'H': 0, 'M': 0, '?': 0}
Entry failures logged: 5
Saved to: /fs/nexus-projects/rlab/atrey/qgqa/augmented-mcqa/results/local_eval/local_eval_smoke_20260220_054548/smoke/smoke__Nanbeige_Nanbeige4_1-3B__opus__full/results/opus/Nanbeige_Nanbeige4.1-3B_gpqa_scratch/3H0M
  FAILED: error: zero successful entries

[44/60] opus_Nanbeige_Nanbeige4.1-3B_gpqa_scratch_3H3M

=== Running Experiment: opus_Nanbeige_Nanbeige4.1-3B_gpqa_scratch_3H3M ===
Model: Local (Nanbeige/Nanbeige4.1-3B)
Config: 3H+3M
Entries: 5
Start index: 0
Checkpoint save interval: 50
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:52:29 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:52:29 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:52:29 [model.py:1745] Using max model len 262144
INFO 02-20 00:52:29 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=0 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:52:30 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:52:30 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:52:30 [model.py:1745] Using max model len 262144
INFO 02-20 00:52:30 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=1 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:52:31 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:52:31 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:52:31 [model.py:1745] Using max model len 262144
INFO 02-20 00:52:31 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=2 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:52:32 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:52:32 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:52:32 [model.py:1745] Using max model len 262144
INFO 02-20 00:52:32 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=3 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:52:33 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:52:34 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:52:34 [model.py:1745] Using max model len 262144
INFO 02-20 00:52:34 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=4 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
üíæ Final checkpoint saved: /fs/nexus-projects/rlab/atrey/qgqa/augmented-mcqa/results/local_eval/local_eval_smoke_20260220_054548/smoke/smoke__Nanbeige_Nanbeige4_1-3B__opus__full/results/opus/Nanbeige_Nanbeige4.1-3B_gpqa_scratch/3H3M/checkpoints/eval_checkpoint_5_20260220_055235.json

=== Results ===
Accuracy (successful entries): 0.00% | attempted=5 successful=0 failed=5
Behavioral: {'G': 0, 'H': 0, 'M': 0, '?': 0}
Entry failures logged: 5
Saved to: /fs/nexus-projects/rlab/atrey/qgqa/augmented-mcqa/results/local_eval/local_eval_smoke_20260220_054548/smoke/smoke__Nanbeige_Nanbeige4_1-3B__opus__full/results/opus/Nanbeige_Nanbeige4.1-3B_gpqa_scratch/3H3M
  FAILED: error: zero successful entries

[45/60] opus_Nanbeige_Nanbeige4.1-3B_gpqa_scratch_3H6M

=== Running Experiment: opus_Nanbeige_Nanbeige4.1-3B_gpqa_scratch_3H6M ===
Model: Local (Nanbeige/Nanbeige4.1-3B)
Config: 3H+6M
Entries: 5
Start index: 0
Checkpoint save interval: 50
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:52:35 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:52:35 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:52:35 [model.py:1745] Using max model len 262144
INFO 02-20 00:52:35 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=0 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:52:36 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:52:36 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:52:36 [model.py:1745] Using max model len 262144
INFO 02-20 00:52:36 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=1 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:52:37 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:52:37 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:52:37 [model.py:1745] Using max model len 262144
INFO 02-20 00:52:37 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=2 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:52:38 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:52:38 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:52:38 [model.py:1745] Using max model len 262144
INFO 02-20 00:52:38 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=3 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:52:39 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:52:39 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:52:39 [model.py:1745] Using max model len 262144
INFO 02-20 00:52:39 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=4 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
üíæ Final checkpoint saved: /fs/nexus-projects/rlab/atrey/qgqa/augmented-mcqa/results/local_eval/local_eval_smoke_20260220_054548/smoke/smoke__Nanbeige_Nanbeige4_1-3B__opus__full/results/opus/Nanbeige_Nanbeige4.1-3B_gpqa_scratch/3H6M/checkpoints/eval_checkpoint_5_20260220_055240.json

=== Results ===
Accuracy (successful entries): 0.00% | attempted=5 successful=0 failed=5
Behavioral: {'G': 0, 'H': 0, 'M': 0, '?': 0}
Entry failures logged: 5
Saved to: /fs/nexus-projects/rlab/atrey/qgqa/augmented-mcqa/results/local_eval/local_eval_smoke_20260220_054548/smoke/smoke__Nanbeige_Nanbeige4_1-3B__opus__full/results/opus/Nanbeige_Nanbeige4.1-3B_gpqa_scratch/3H6M
  FAILED: error: zero successful entries

[46/60] opus_Nanbeige_Nanbeige4.1-3B_mmlu_pro_dhuman_0H3M

=== Running Experiment: opus_Nanbeige_Nanbeige4.1-3B_mmlu_pro_dhuman_0H3M ===
Model: Local (Nanbeige/Nanbeige4.1-3B)
Config: 0H+3M
Entries: 5
Start index: 0
Checkpoint save interval: 50
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:52:41 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:52:41 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:52:41 [model.py:1745] Using max model len 262144
INFO 02-20 00:52:41 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=0 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:52:42 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:52:42 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:52:42 [model.py:1745] Using max model len 262144
INFO 02-20 00:52:42 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=1 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:52:43 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:52:43 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:52:43 [model.py:1745] Using max model len 262144
INFO 02-20 00:52:43 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=2 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:52:44 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:52:44 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:52:44 [model.py:1745] Using max model len 262144
INFO 02-20 00:52:44 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=3 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:52:45 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:52:45 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:52:45 [model.py:1745] Using max model len 262144
INFO 02-20 00:52:45 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=4 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
üíæ Final checkpoint saved: /fs/nexus-projects/rlab/atrey/qgqa/augmented-mcqa/results/local_eval/local_eval_smoke_20260220_054548/smoke/smoke__Nanbeige_Nanbeige4_1-3B__opus__full/results/opus/Nanbeige_Nanbeige4.1-3B_mmlu_pro_dhuman/0H3M/checkpoints/eval_checkpoint_5_20260220_055246.json

=== Results ===
Accuracy (successful entries): 0.00% | attempted=5 successful=0 failed=5
Behavioral: {'G': 0, 'H': 0, 'M': 0, '?': 0}
Entry failures logged: 5
Saved to: /fs/nexus-projects/rlab/atrey/qgqa/augmented-mcqa/results/local_eval/local_eval_smoke_20260220_054548/smoke/smoke__Nanbeige_Nanbeige4_1-3B__opus__full/results/opus/Nanbeige_Nanbeige4.1-3B_mmlu_pro_dhuman/0H3M
  FAILED: error: zero successful entries

[47/60] opus_Nanbeige_Nanbeige4.1-3B_mmlu_pro_dhuman_0H6M

=== Running Experiment: opus_Nanbeige_Nanbeige4.1-3B_mmlu_pro_dhuman_0H6M ===
Model: Local (Nanbeige/Nanbeige4.1-3B)
Config: 0H+6M
Entries: 5
Start index: 0
Checkpoint save interval: 50
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:52:46 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:52:47 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:52:47 [model.py:1745] Using max model len 262144
INFO 02-20 00:52:47 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=0 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:52:48 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:52:48 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:52:48 [model.py:1745] Using max model len 262144
INFO 02-20 00:52:48 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=1 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:52:49 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:52:49 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:52:49 [model.py:1745] Using max model len 262144
INFO 02-20 00:52:49 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=2 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:52:50 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:52:50 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:52:50 [model.py:1745] Using max model len 262144
INFO 02-20 00:52:50 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=3 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:52:51 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:52:51 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:52:51 [model.py:1745] Using max model len 262144
INFO 02-20 00:52:51 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=4 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
üíæ Final checkpoint saved: /fs/nexus-projects/rlab/atrey/qgqa/augmented-mcqa/results/local_eval/local_eval_smoke_20260220_054548/smoke/smoke__Nanbeige_Nanbeige4_1-3B__opus__full/results/opus/Nanbeige_Nanbeige4.1-3B_mmlu_pro_dhuman/0H6M/checkpoints/eval_checkpoint_5_20260220_055252.json

=== Results ===
Accuracy (successful entries): 0.00% | attempted=5 successful=0 failed=5
Behavioral: {'G': 0, 'H': 0, 'M': 0, '?': 0}
Entry failures logged: 5
Saved to: /fs/nexus-projects/rlab/atrey/qgqa/augmented-mcqa/results/local_eval/local_eval_smoke_20260220_054548/smoke/smoke__Nanbeige_Nanbeige4_1-3B__opus__full/results/opus/Nanbeige_Nanbeige4.1-3B_mmlu_pro_dhuman/0H6M
  FAILED: error: zero successful entries

[48/60] opus_Nanbeige_Nanbeige4.1-3B_mmlu_pro_dhuman_3H0M

=== Running Experiment: opus_Nanbeige_Nanbeige4.1-3B_mmlu_pro_dhuman_3H0M ===
Model: Local (Nanbeige/Nanbeige4.1-3B)
Config: 3H+0M
Entries: 5
Start index: 0
Checkpoint save interval: 50
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:52:52 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:52:53 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:52:53 [model.py:1745] Using max model len 262144
INFO 02-20 00:52:53 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=0 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:52:54 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:52:54 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:52:54 [model.py:1745] Using max model len 262144
INFO 02-20 00:52:54 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=1 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:52:55 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:52:55 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:52:55 [model.py:1745] Using max model len 262144
INFO 02-20 00:52:55 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=2 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:52:56 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:52:56 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:52:56 [model.py:1745] Using max model len 262144
INFO 02-20 00:52:56 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=3 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:52:57 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:52:57 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:52:57 [model.py:1745] Using max model len 262144
INFO 02-20 00:52:57 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=4 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
üíæ Final checkpoint saved: /fs/nexus-projects/rlab/atrey/qgqa/augmented-mcqa/results/local_eval/local_eval_smoke_20260220_054548/smoke/smoke__Nanbeige_Nanbeige4_1-3B__opus__full/results/opus/Nanbeige_Nanbeige4.1-3B_mmlu_pro_dhuman/3H0M/checkpoints/eval_checkpoint_5_20260220_055258.json

=== Results ===
Accuracy (successful entries): 0.00% | attempted=5 successful=0 failed=5
Behavioral: {'G': 0, 'H': 0, 'M': 0, '?': 0}
Entry failures logged: 5
Saved to: /fs/nexus-projects/rlab/atrey/qgqa/augmented-mcqa/results/local_eval/local_eval_smoke_20260220_054548/smoke/smoke__Nanbeige_Nanbeige4_1-3B__opus__full/results/opus/Nanbeige_Nanbeige4.1-3B_mmlu_pro_dhuman/3H0M
  FAILED: error: zero successful entries

[49/60] opus_Nanbeige_Nanbeige4.1-3B_mmlu_pro_dhuman_3H3M

=== Running Experiment: opus_Nanbeige_Nanbeige4.1-3B_mmlu_pro_dhuman_3H3M ===
Model: Local (Nanbeige/Nanbeige4.1-3B)
Config: 3H+3M
Entries: 5
Start index: 0
Checkpoint save interval: 50
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:52:58 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:52:58 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:52:58 [model.py:1745] Using max model len 262144
INFO 02-20 00:52:58 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=0 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:52:59 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:53:00 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:53:00 [model.py:1745] Using max model len 262144
INFO 02-20 00:53:00 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=1 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:53:01 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:53:01 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:53:01 [model.py:1745] Using max model len 262144
INFO 02-20 00:53:01 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=2 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:53:02 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:53:02 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:53:02 [model.py:1745] Using max model len 262144
INFO 02-20 00:53:02 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=3 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:53:03 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:53:03 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:53:03 [model.py:1745] Using max model len 262144
INFO 02-20 00:53:03 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=4 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
üíæ Final checkpoint saved: /fs/nexus-projects/rlab/atrey/qgqa/augmented-mcqa/results/local_eval/local_eval_smoke_20260220_054548/smoke/smoke__Nanbeige_Nanbeige4_1-3B__opus__full/results/opus/Nanbeige_Nanbeige4.1-3B_mmlu_pro_dhuman/3H3M/checkpoints/eval_checkpoint_5_20260220_055304.json

=== Results ===
Accuracy (successful entries): 0.00% | attempted=5 successful=0 failed=5
Behavioral: {'G': 0, 'H': 0, 'M': 0, '?': 0}
Entry failures logged: 5
Saved to: /fs/nexus-projects/rlab/atrey/qgqa/augmented-mcqa/results/local_eval/local_eval_smoke_20260220_054548/smoke/smoke__Nanbeige_Nanbeige4_1-3B__opus__full/results/opus/Nanbeige_Nanbeige4.1-3B_mmlu_pro_dhuman/3H3M
  FAILED: error: zero successful entries

[50/60] opus_Nanbeige_Nanbeige4.1-3B_mmlu_pro_dhuman_3H6M

=== Running Experiment: opus_Nanbeige_Nanbeige4.1-3B_mmlu_pro_dhuman_3H6M ===
Model: Local (Nanbeige/Nanbeige4.1-3B)
Config: 3H+6M
Entries: 5
Start index: 0
Checkpoint save interval: 50
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:53:04 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:53:04 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:53:04 [model.py:1745] Using max model len 262144
INFO 02-20 00:53:04 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=0 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:53:05 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:53:05 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:53:05 [model.py:1745] Using max model len 262144
INFO 02-20 00:53:05 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=1 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:53:06 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:53:07 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:53:07 [model.py:1745] Using max model len 262144
INFO 02-20 00:53:07 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=2 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:53:08 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:53:08 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:53:08 [model.py:1745] Using max model len 262144
INFO 02-20 00:53:08 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=3 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:53:09 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:53:09 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:53:09 [model.py:1745] Using max model len 262144
INFO 02-20 00:53:09 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=4 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
üíæ Final checkpoint saved: /fs/nexus-projects/rlab/atrey/qgqa/augmented-mcqa/results/local_eval/local_eval_smoke_20260220_054548/smoke/smoke__Nanbeige_Nanbeige4_1-3B__opus__full/results/opus/Nanbeige_Nanbeige4.1-3B_mmlu_pro_dhuman/3H6M/checkpoints/eval_checkpoint_5_20260220_055310.json

=== Results ===
Accuracy (successful entries): 0.00% | attempted=5 successful=0 failed=5
Behavioral: {'G': 0, 'H': 0, 'M': 0, '?': 0}
Entry failures logged: 5
Saved to: /fs/nexus-projects/rlab/atrey/qgqa/augmented-mcqa/results/local_eval/local_eval_smoke_20260220_054548/smoke/smoke__Nanbeige_Nanbeige4_1-3B__opus__full/results/opus/Nanbeige_Nanbeige4.1-3B_mmlu_pro_dhuman/3H6M
  FAILED: error: zero successful entries

[51/60] opus_Nanbeige_Nanbeige4.1-3B_mmlu_pro_dmodel_0H3M

=== Running Experiment: opus_Nanbeige_Nanbeige4.1-3B_mmlu_pro_dmodel_0H3M ===
Model: Local (Nanbeige/Nanbeige4.1-3B)
Config: 0H+3M
Entries: 5
Start index: 0
Checkpoint save interval: 50
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:53:10 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:53:10 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:53:10 [model.py:1745] Using max model len 262144
INFO 02-20 00:53:10 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=0 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:53:11 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:53:12 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:53:12 [model.py:1745] Using max model len 262144
INFO 02-20 00:53:12 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=1 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:53:13 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:53:13 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:53:13 [model.py:1745] Using max model len 262144
INFO 02-20 00:53:13 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=2 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:53:14 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:53:14 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:53:14 [model.py:1745] Using max model len 262144
INFO 02-20 00:53:14 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=3 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:53:15 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:53:15 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:53:15 [model.py:1745] Using max model len 262144
INFO 02-20 00:53:15 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=4 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
üíæ Final checkpoint saved: /fs/nexus-projects/rlab/atrey/qgqa/augmented-mcqa/results/local_eval/local_eval_smoke_20260220_054548/smoke/smoke__Nanbeige_Nanbeige4_1-3B__opus__full/results/opus/Nanbeige_Nanbeige4.1-3B_mmlu_pro_dmodel/0H3M/checkpoints/eval_checkpoint_5_20260220_055316.json

=== Results ===
Accuracy (successful entries): 0.00% | attempted=5 successful=0 failed=5
Behavioral: {'G': 0, 'H': 0, 'M': 0, '?': 0}
Entry failures logged: 5
Saved to: /fs/nexus-projects/rlab/atrey/qgqa/augmented-mcqa/results/local_eval/local_eval_smoke_20260220_054548/smoke/smoke__Nanbeige_Nanbeige4_1-3B__opus__full/results/opus/Nanbeige_Nanbeige4.1-3B_mmlu_pro_dmodel/0H3M
  FAILED: error: zero successful entries

[52/60] opus_Nanbeige_Nanbeige4.1-3B_mmlu_pro_dmodel_0H6M

=== Running Experiment: opus_Nanbeige_Nanbeige4.1-3B_mmlu_pro_dmodel_0H6M ===
Model: Local (Nanbeige/Nanbeige4.1-3B)
Config: 0H+6M
Entries: 5
Start index: 0
Checkpoint save interval: 50
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:53:16 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:53:16 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:53:16 [model.py:1745] Using max model len 262144
INFO 02-20 00:53:16 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=0 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:53:17 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:53:17 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:53:17 [model.py:1745] Using max model len 262144
INFO 02-20 00:53:17 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=1 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:53:18 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:53:19 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:53:19 [model.py:1745] Using max model len 262144
INFO 02-20 00:53:19 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=2 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:53:20 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:53:20 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:53:20 [model.py:1745] Using max model len 262144
INFO 02-20 00:53:20 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=3 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:53:21 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:53:21 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:53:21 [model.py:1745] Using max model len 262144
INFO 02-20 00:53:21 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=4 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
üíæ Final checkpoint saved: /fs/nexus-projects/rlab/atrey/qgqa/augmented-mcqa/results/local_eval/local_eval_smoke_20260220_054548/smoke/smoke__Nanbeige_Nanbeige4_1-3B__opus__full/results/opus/Nanbeige_Nanbeige4.1-3B_mmlu_pro_dmodel/0H6M/checkpoints/eval_checkpoint_5_20260220_055322.json

=== Results ===
Accuracy (successful entries): 0.00% | attempted=5 successful=0 failed=5
Behavioral: {'G': 0, 'H': 0, 'M': 0, '?': 0}
Entry failures logged: 5
Saved to: /fs/nexus-projects/rlab/atrey/qgqa/augmented-mcqa/results/local_eval/local_eval_smoke_20260220_054548/smoke/smoke__Nanbeige_Nanbeige4_1-3B__opus__full/results/opus/Nanbeige_Nanbeige4.1-3B_mmlu_pro_dmodel/0H6M
  FAILED: error: zero successful entries

[53/60] opus_Nanbeige_Nanbeige4.1-3B_mmlu_pro_dmodel_3H0M

=== Running Experiment: opus_Nanbeige_Nanbeige4.1-3B_mmlu_pro_dmodel_3H0M ===
Model: Local (Nanbeige/Nanbeige4.1-3B)
Config: 3H+0M
Entries: 5
Start index: 0
Checkpoint save interval: 50
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:53:22 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:53:22 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:53:22 [model.py:1745] Using max model len 262144
INFO 02-20 00:53:22 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=0 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:53:23 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:53:23 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:53:23 [model.py:1745] Using max model len 262144
INFO 02-20 00:53:23 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=1 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:53:24 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:53:25 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:53:25 [model.py:1745] Using max model len 262144
INFO 02-20 00:53:25 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=2 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:53:26 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:53:26 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:53:26 [model.py:1745] Using max model len 262144
INFO 02-20 00:53:26 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=3 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:53:27 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:53:27 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:53:27 [model.py:1745] Using max model len 262144
INFO 02-20 00:53:27 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=4 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
üíæ Final checkpoint saved: /fs/nexus-projects/rlab/atrey/qgqa/augmented-mcqa/results/local_eval/local_eval_smoke_20260220_054548/smoke/smoke__Nanbeige_Nanbeige4_1-3B__opus__full/results/opus/Nanbeige_Nanbeige4.1-3B_mmlu_pro_dmodel/3H0M/checkpoints/eval_checkpoint_5_20260220_055328.json

=== Results ===
Accuracy (successful entries): 0.00% | attempted=5 successful=0 failed=5
Behavioral: {'G': 0, 'H': 0, 'M': 0, '?': 0}
Entry failures logged: 5
Saved to: /fs/nexus-projects/rlab/atrey/qgqa/augmented-mcqa/results/local_eval/local_eval_smoke_20260220_054548/smoke/smoke__Nanbeige_Nanbeige4_1-3B__opus__full/results/opus/Nanbeige_Nanbeige4.1-3B_mmlu_pro_dmodel/3H0M
  FAILED: error: zero successful entries

[54/60] opus_Nanbeige_Nanbeige4.1-3B_mmlu_pro_dmodel_3H3M

=== Running Experiment: opus_Nanbeige_Nanbeige4.1-3B_mmlu_pro_dmodel_3H3M ===
Model: Local (Nanbeige/Nanbeige4.1-3B)
Config: 3H+3M
Entries: 5
Start index: 0
Checkpoint save interval: 50
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:53:28 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:53:28 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:53:28 [model.py:1745] Using max model len 262144
INFO 02-20 00:53:28 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=0 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:53:29 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:53:29 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:53:29 [model.py:1745] Using max model len 262144
INFO 02-20 00:53:29 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=1 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:53:30 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:53:31 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:53:31 [model.py:1745] Using max model len 262144
INFO 02-20 00:53:31 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=2 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:53:32 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:53:32 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:53:32 [model.py:1745] Using max model len 262144
INFO 02-20 00:53:32 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=3 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:53:33 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:53:33 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:53:33 [model.py:1745] Using max model len 262144
INFO 02-20 00:53:33 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=4 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
üíæ Final checkpoint saved: /fs/nexus-projects/rlab/atrey/qgqa/augmented-mcqa/results/local_eval/local_eval_smoke_20260220_054548/smoke/smoke__Nanbeige_Nanbeige4_1-3B__opus__full/results/opus/Nanbeige_Nanbeige4.1-3B_mmlu_pro_dmodel/3H3M/checkpoints/eval_checkpoint_5_20260220_055334.json

=== Results ===
Accuracy (successful entries): 0.00% | attempted=5 successful=0 failed=5
Behavioral: {'G': 0, 'H': 0, 'M': 0, '?': 0}
Entry failures logged: 5
Saved to: /fs/nexus-projects/rlab/atrey/qgqa/augmented-mcqa/results/local_eval/local_eval_smoke_20260220_054548/smoke/smoke__Nanbeige_Nanbeige4_1-3B__opus__full/results/opus/Nanbeige_Nanbeige4.1-3B_mmlu_pro_dmodel/3H3M
  FAILED: error: zero successful entries

[55/60] opus_Nanbeige_Nanbeige4.1-3B_mmlu_pro_dmodel_3H6M

=== Running Experiment: opus_Nanbeige_Nanbeige4.1-3B_mmlu_pro_dmodel_3H6M ===
Model: Local (Nanbeige/Nanbeige4.1-3B)
Config: 3H+6M
Entries: 5
Start index: 0
Checkpoint save interval: 50
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:53:34 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:53:34 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:53:34 [model.py:1745] Using max model len 262144
INFO 02-20 00:53:34 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=0 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:53:35 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:53:36 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:53:36 [model.py:1745] Using max model len 262144
INFO 02-20 00:53:36 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=1 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:53:37 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:53:37 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:53:37 [model.py:1745] Using max model len 262144
INFO 02-20 00:53:37 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=2 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:53:38 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:53:38 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:53:38 [model.py:1745] Using max model len 262144
INFO 02-20 00:53:38 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=3 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:53:39 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:53:39 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:53:39 [model.py:1745] Using max model len 262144
INFO 02-20 00:53:39 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=4 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
üíæ Final checkpoint saved: /fs/nexus-projects/rlab/atrey/qgqa/augmented-mcqa/results/local_eval/local_eval_smoke_20260220_054548/smoke/smoke__Nanbeige_Nanbeige4_1-3B__opus__full/results/opus/Nanbeige_Nanbeige4.1-3B_mmlu_pro_dmodel/3H6M/checkpoints/eval_checkpoint_5_20260220_055340.json

=== Results ===
Accuracy (successful entries): 0.00% | attempted=5 successful=0 failed=5
Behavioral: {'G': 0, 'H': 0, 'M': 0, '?': 0}
Entry failures logged: 5
Saved to: /fs/nexus-projects/rlab/atrey/qgqa/augmented-mcqa/results/local_eval/local_eval_smoke_20260220_054548/smoke/smoke__Nanbeige_Nanbeige4_1-3B__opus__full/results/opus/Nanbeige_Nanbeige4.1-3B_mmlu_pro_dmodel/3H6M
  FAILED: error: zero successful entries

[56/60] opus_Nanbeige_Nanbeige4.1-3B_mmlu_pro_scratch_0H3M

=== Running Experiment: opus_Nanbeige_Nanbeige4.1-3B_mmlu_pro_scratch_0H3M ===
Model: Local (Nanbeige/Nanbeige4.1-3B)
Config: 0H+3M
Entries: 5
Start index: 0
Checkpoint save interval: 50
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:53:40 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:53:40 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:53:40 [model.py:1745] Using max model len 262144
INFO 02-20 00:53:40 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=0 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:53:41 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:53:42 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:53:42 [model.py:1745] Using max model len 262144
INFO 02-20 00:53:42 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=1 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:53:43 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:53:43 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:53:43 [model.py:1745] Using max model len 262144
INFO 02-20 00:53:43 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=2 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:53:44 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:53:44 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:53:44 [model.py:1745] Using max model len 262144
INFO 02-20 00:53:44 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=3 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:53:45 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:53:45 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:53:45 [model.py:1745] Using max model len 262144
INFO 02-20 00:53:45 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=4 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
üíæ Final checkpoint saved: /fs/nexus-projects/rlab/atrey/qgqa/augmented-mcqa/results/local_eval/local_eval_smoke_20260220_054548/smoke/smoke__Nanbeige_Nanbeige4_1-3B__opus__full/results/opus/Nanbeige_Nanbeige4.1-3B_mmlu_pro_scratch/0H3M/checkpoints/eval_checkpoint_5_20260220_055346.json

=== Results ===
Accuracy (successful entries): 0.00% | attempted=5 successful=0 failed=5
Behavioral: {'G': 0, 'H': 0, 'M': 0, '?': 0}
Entry failures logged: 5
Saved to: /fs/nexus-projects/rlab/atrey/qgqa/augmented-mcqa/results/local_eval/local_eval_smoke_20260220_054548/smoke/smoke__Nanbeige_Nanbeige4_1-3B__opus__full/results/opus/Nanbeige_Nanbeige4.1-3B_mmlu_pro_scratch/0H3M
  FAILED: error: zero successful entries

[57/60] opus_Nanbeige_Nanbeige4.1-3B_mmlu_pro_scratch_0H6M

=== Running Experiment: opus_Nanbeige_Nanbeige4.1-3B_mmlu_pro_scratch_0H6M ===
Model: Local (Nanbeige/Nanbeige4.1-3B)
Config: 0H+6M
Entries: 5
Start index: 0
Checkpoint save interval: 50
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:53:46 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:53:47 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:53:47 [model.py:1745] Using max model len 262144
INFO 02-20 00:53:47 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=0 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:53:48 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:53:48 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:53:48 [model.py:1745] Using max model len 262144
INFO 02-20 00:53:48 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=1 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:53:49 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:53:49 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:53:49 [model.py:1745] Using max model len 262144
INFO 02-20 00:53:49 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=2 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:53:50 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:53:50 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:53:50 [model.py:1745] Using max model len 262144
INFO 02-20 00:53:50 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=3 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:53:51 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:53:51 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:53:51 [model.py:1745] Using max model len 262144
INFO 02-20 00:53:51 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=4 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
üíæ Final checkpoint saved: /fs/nexus-projects/rlab/atrey/qgqa/augmented-mcqa/results/local_eval/local_eval_smoke_20260220_054548/smoke/smoke__Nanbeige_Nanbeige4_1-3B__opus__full/results/opus/Nanbeige_Nanbeige4.1-3B_mmlu_pro_scratch/0H6M/checkpoints/eval_checkpoint_5_20260220_055352.json

=== Results ===
Accuracy (successful entries): 0.00% | attempted=5 successful=0 failed=5
Behavioral: {'G': 0, 'H': 0, 'M': 0, '?': 0}
Entry failures logged: 5
Saved to: /fs/nexus-projects/rlab/atrey/qgqa/augmented-mcqa/results/local_eval/local_eval_smoke_20260220_054548/smoke/smoke__Nanbeige_Nanbeige4_1-3B__opus__full/results/opus/Nanbeige_Nanbeige4.1-3B_mmlu_pro_scratch/0H6M
  FAILED: error: zero successful entries

[58/60] opus_Nanbeige_Nanbeige4.1-3B_mmlu_pro_scratch_3H0M

=== Running Experiment: opus_Nanbeige_Nanbeige4.1-3B_mmlu_pro_scratch_3H0M ===
Model: Local (Nanbeige/Nanbeige4.1-3B)
Config: 3H+0M
Entries: 5
Start index: 0
Checkpoint save interval: 50
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:53:52 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:53:53 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:53:53 [model.py:1745] Using max model len 262144
INFO 02-20 00:53:53 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=0 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:53:54 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:53:54 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:53:54 [model.py:1745] Using max model len 262144
INFO 02-20 00:53:54 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=1 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:53:55 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:53:55 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:53:55 [model.py:1745] Using max model len 262144
INFO 02-20 00:53:55 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=2 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:53:56 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:53:56 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:53:56 [model.py:1745] Using max model len 262144
INFO 02-20 00:53:56 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=3 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:53:57 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:53:57 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:53:57 [model.py:1745] Using max model len 262144
INFO 02-20 00:53:57 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=4 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
üíæ Final checkpoint saved: /fs/nexus-projects/rlab/atrey/qgqa/augmented-mcqa/results/local_eval/local_eval_smoke_20260220_054548/smoke/smoke__Nanbeige_Nanbeige4_1-3B__opus__full/results/opus/Nanbeige_Nanbeige4.1-3B_mmlu_pro_scratch/3H0M/checkpoints/eval_checkpoint_5_20260220_055358.json

=== Results ===
Accuracy (successful entries): 0.00% | attempted=5 successful=0 failed=5
Behavioral: {'G': 0, 'H': 0, 'M': 0, '?': 0}
Entry failures logged: 5
Saved to: /fs/nexus-projects/rlab/atrey/qgqa/augmented-mcqa/results/local_eval/local_eval_smoke_20260220_054548/smoke/smoke__Nanbeige_Nanbeige4_1-3B__opus__full/results/opus/Nanbeige_Nanbeige4.1-3B_mmlu_pro_scratch/3H0M
  FAILED: error: zero successful entries

[59/60] opus_Nanbeige_Nanbeige4.1-3B_mmlu_pro_scratch_3H3M

=== Running Experiment: opus_Nanbeige_Nanbeige4.1-3B_mmlu_pro_scratch_3H3M ===
Model: Local (Nanbeige/Nanbeige4.1-3B)
Config: 3H+3M
Entries: 5
Start index: 0
Checkpoint save interval: 50
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:53:58 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:53:58 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:53:58 [model.py:1745] Using max model len 262144
INFO 02-20 00:53:58 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=0 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:53:59 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:54:00 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:54:00 [model.py:1745] Using max model len 262144
INFO 02-20 00:54:00 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=1 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:54:01 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:54:01 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:54:01 [model.py:1745] Using max model len 262144
INFO 02-20 00:54:01 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=2 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:54:02 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:54:02 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:54:02 [model.py:1745] Using max model len 262144
INFO 02-20 00:54:02 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=3 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:54:03 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:54:03 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:54:03 [model.py:1745] Using max model len 262144
INFO 02-20 00:54:03 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=4 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
üíæ Final checkpoint saved: /fs/nexus-projects/rlab/atrey/qgqa/augmented-mcqa/results/local_eval/local_eval_smoke_20260220_054548/smoke/smoke__Nanbeige_Nanbeige4_1-3B__opus__full/results/opus/Nanbeige_Nanbeige4.1-3B_mmlu_pro_scratch/3H3M/checkpoints/eval_checkpoint_5_20260220_055404.json

=== Results ===
Accuracy (successful entries): 0.00% | attempted=5 successful=0 failed=5
Behavioral: {'G': 0, 'H': 0, 'M': 0, '?': 0}
Entry failures logged: 5
Saved to: /fs/nexus-projects/rlab/atrey/qgqa/augmented-mcqa/results/local_eval/local_eval_smoke_20260220_054548/smoke/smoke__Nanbeige_Nanbeige4_1-3B__opus__full/results/opus/Nanbeige_Nanbeige4.1-3B_mmlu_pro_scratch/3H3M
  FAILED: error: zero successful entries

[60/60] opus_Nanbeige_Nanbeige4.1-3B_mmlu_pro_scratch_3H6M

=== Running Experiment: opus_Nanbeige_Nanbeige4.1-3B_mmlu_pro_scratch_3H6M ===
Model: Local (Nanbeige/Nanbeige4.1-3B)
Config: 3H+6M
Entries: 5
Start index: 0
Checkpoint save interval: 50
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:54:04 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:54:04 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:54:04 [model.py:1745] Using max model len 262144
INFO 02-20 00:54:04 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=0 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:54:05 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:54:06 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:54:06 [model.py:1745] Using max model len 262144
INFO 02-20 00:54:06 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=1 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:54:07 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:54:07 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:54:07 [model.py:1745] Using max model len 262144
INFO 02-20 00:54:07 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=2 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:54:08 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:54:08 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:54:08 [model.py:1745] Using max model len 262144
INFO 02-20 00:54:08 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=3 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
Loading model: Nanbeige/Nanbeige4.1-3B
  GPU memory utilization: 0.9
  Tensor parallel size: 1
  Seed: 12345
  Dtype: auto
  Tokenizer mode: slow
INFO 02-20 00:54:09 [utils.py:253] non-default args: {'tokenizer_mode': 'slow', 'trust_remote_code': True, 'download_dir': '/fs/nexus-scratch/adesai10/hub', 'seed': 12345, 'disable_log_stats': True, 'model': 'Nanbeige/Nanbeige4.1-3B'}
INFO 02-20 00:54:09 [model.py:631] Resolved architecture: LlamaForCausalLM
INFO 02-20 00:54:09 [model.py:1745] Using max model len 262144
INFO 02-20 00:54:09 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
‚ö†Ô∏è Entry failed but continuing | idx=4 stage=model_generate type=AttributeError: TokenizersBackend has no attribute all_special_tokens_extended
üíæ Final checkpoint saved: /fs/nexus-projects/rlab/atrey/qgqa/augmented-mcqa/results/local_eval/local_eval_smoke_20260220_054548/smoke/smoke__Nanbeige_Nanbeige4_1-3B__opus__full/results/opus/Nanbeige_Nanbeige4.1-3B_mmlu_pro_scratch/3H6M/checkpoints/eval_checkpoint_5_20260220_055411.json

=== Results ===
Accuracy (successful entries): 0.00% | attempted=5 successful=0 failed=5
Behavioral: {'G': 0, 'H': 0, 'M': 0, '?': 0}
Entry failures logged: 5
Saved to: /fs/nexus-projects/rlab/atrey/qgqa/augmented-mcqa/results/local_eval/local_eval_smoke_20260220_054548/smoke/smoke__Nanbeige_Nanbeige4_1-3B__opus__full/results/opus/Nanbeige_Nanbeige4.1-3B_mmlu_pro_scratch/3H6M
  FAILED: error: zero successful entries
