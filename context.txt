# Augmented MCQA - Project Context

## Overview

Research framework for studying model-generated distractors in multiple choice question answering (MCQA). The core research question: how do synthetic distractors compare to human-written ones in terms of model performance?

## Repository Structure

```
augmented-mcqa/
├── config/              # Central configuration
│   ├── settings.py      # Paths, API keys, enums (DistractorType, DatasetType)
│   └── __init__.py      # Exports all config
├── data/                # Data pipeline
│   ├── downloader.py    # Download datasets from HF to datasets/raw/
│   ├── sorter.py        # Sort MMLU-Pro to datasets/processed/
│   ├── arc_processor.py # Process ARC to datasets/processed/
│   ├── supergpqa_processor.py  # Process SuperGPQA to datasets/processed/
│   ├── augmentor.py     # Generate synthetic distractors to datasets/augmented/
│   ├── filter.py        # Create filtered subsets (3H0M, 0H3M, etc.)
│   └── adapter.py       # Unified data access layer (DataAdapter)
├── models/              # LLM clients (SINGLE SOURCE OF TRUTH)
│   ├── base.py          # Abstract ModelClient interface
│   ├── __init__.py      # Factory (get_client) & Registry
│   ├── openai_client.py
│   ├── anthropic_client.py
│   ├── gemini_client.py
│   ├── deepseek_client.py
│   └── local_client.py
├── experiments/         # Experiment framework
│   ├── config.py        # ExperimentConfig dataclass
│   ├── runner.py        # ExperimentRunner
│   └── difficulty.py    # Difficulty scaling experiments
├── evaluation/          # Answer extraction and scoring
│   └── evaluator.py     # build_mcqa_prompt (SINGLE SOURCE OF TRUTH)
├── analysis/            # Analysis and visualization
│   ├── analyzer.py      # Behavioral signatures
│   └── visualize.py     # Plotting functions
├── scripts/             # CLI entry points
│   ├── process_all.py           # Process all raw datasets
│   ├── generate_distractors.py  # Generate augmented datasets
│   └── run_experiment.py        # Run evaluation experiments
├── tests/               # Test Suite
│   ├── conftest.py
│   ├── test_config.py
│   ├── test_models.py
│   ├── test_data_processing.py
│   ├── test_augmentation.py
│   └── test_integration.py
├── datasets/            # Data Directory (Managed Structure)
│   ├── raw/             # Downloaded from HuggingFace
│   ├── processed/       # Unified JSON format
│   └── augmented/       # Generated distractors
│       ├── from_scratch/
│       ├── conditioned_human/
│       └── conditioned_synthetic/
└── results/             # Experiment outputs
```

## Key Concepts

### Distractor Types (Unified Enum)
| Column | Meaning |
|--------|---------|
| `cond_human_q_a` | Human-written distractors (from original MMLU) |
| `cond_model_q_a` | Model-generated from Q+A only |
| `cond_model_q_a_dhuman` | Model-generated conditioned on human distractors |
| `cond_model_q_a_dmodel` | Model-generated conditioned on model distractors |

### Datasets
1. **MMLU-Pro** - Primary dataset, has both human and synthetic distractors
2. **ARC** - Easy and Challenge splits
3. **SuperGPQA** - Filtered to 10-option questions only

## Pipeline

1. **Download**: `python -m data.downloader --dataset all` -> `datasets/raw/`
2. **Process**: `python scripts/process_all.py` -> `datasets/processed/`
3. **Generate**: `python scripts/generate_distractors.py ...` -> `datasets/augmented/{mode}/`
4. **Filter**: `data.filter` tools
5. **Experiment**: `python scripts/run_experiment.py ...`

## Model Registry

Use `models.get_client(model_name)` to get a client instance.
Registry defined in `models/__init__.py`.

## Testing

Run the full test suite with `pytest`:
```bash
python -m pytest tests/
```

## Environment

- API keys in `.env`
- Paths: `DATASETS_DIR` (defaults to `./datasets`), `RESULTS_DIR`
- Conda env: `qgqa`

# Project Context: Augmented MCQA

## Current Status
Transitioning to standardizing all processed datasets as Hugging Face `Dataset` objects (saving via `save_to_disk` and pushing to Hub). 

## Major Changes & Refactoring

### 1. Robust/Exact MMLU-Pro Matching
- Implemented **Exact Matching** between MMLU and MMLU-Pro questions using raw strings (before any cleaning).
- Only questions common to both datasets are preserved.
- Human vs Synthetic distractors are identified based on exact option matches in the original MMLU.
- Post-identification, whitespace cleaning and STEM-specific bug fixes (leading whitespace removal for chemistry, physics, math) are applied.

### 2. Dataset Standardization (HF Dataset Format)
- **MMLU-Pro**: Processed as `datasets.Dataset`, renamed `test` split to `train`, and removed `validation` split. Saved locally via `save_to_disk`.
- **ARC (Easy & Challenge)**: Updated `data/arc_processor.py` to filter questions with fewer than 4 options. Converted to `datasets.Dataset` format and saved locally using `save_to_disk` (instead of JSON).
- **SuperGPQA**: Filtered to 10-option questions. Converted to `datasets.Dataset` format and saved locally using `save_to_disk`.

### 3. Pipeline Refactoring
- **Consolidation**: Integrated `sorter.py` logic into `mmlu_pro_processor.py`.
- **Deletion**: Removed obsolete `adapter.py` and `sorter.py`.
- **Master Script**: Updated `scripts/process_all.py` to orchestration processing of all three datasets with directory-based output paths to support the HF Dataset format. Added `--limit` support.

### 4. Hugging Face Integration
- All processed datasets are automatically pushed to the Hugging Face Hub under the `atreydesai/` namespace as proper datasets.

## Key Files
- `data/mmlu_pro_processor.py`: Exact matching and split refactoring logic.
- `data/arc_processor.py`: Minimum 4-option filter and HF Dataset conversion.
- `data/supergpqa_processor.py`: HF Dataset conversion.
- `scripts/process_all.py`: Orchestration and path management.
- `data/hub_utils.py`: Centralized Hub pushing utility.

## Deployment Details
- Processed datasets saved to `datasets/processed/`.
- Exact directory names:
    - `datasets/processed/mmlu_pro_processed/`
    - `datasets/processed/arc_processed/arc_easy/`
    - `datasets/processed/arc_processed/arc_challenge/`
    - `datasets/processed/supergpqa_processed/`
