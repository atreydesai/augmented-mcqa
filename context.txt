# Augmented MCQA - Project Context

## Overview

Research framework for studying model-generated distractors in multiple choice question answering (MCQA). The core research question: how do synthetic distractors compare to human-written ones in terms of model performance?

## Repository Structure

```
augmented-mcqa/
├── config/              # Central configuration
│   ├── settings.py      # Paths, API keys, enums (DistractorType, DatasetType)
│   └── __init__.py      # Exports all config
├── data/                # Data pipeline
│   ├── downloader.py    # Download datasets from HF to datasets/raw/
│   ├── sorter.py        # Sort MMLU-Pro to datasets/processed/
│   ├── arc_processor.py # Process ARC to datasets/processed/
│   ├── supergpqa_processor.py  # Process SuperGPQA to datasets/processed/
│   ├── augmentor.py     # Generate synthetic distractors to datasets/augmented/
│   ├── filter.py        # Create filtered subsets (3H0M, 0H3M, etc.)
│   └── adapter.py       # Unified data access layer (DataAdapter)
├── models/              # LLM clients (SINGLE SOURCE OF TRUTH)
│   ├── base.py          # Abstract ModelClient interface
│   ├── __init__.py      # Factory (get_client) & Registry
│   ├── openai_client.py
│   ├── anthropic_client.py
│   ├── gemini_client.py
│   ├── deepseek_client.py
│   └── local_client.py
├── experiments/         # Experiment framework
│   ├── config.py        # ExperimentConfig dataclass
│   ├── runner.py        # ExperimentRunner
│   └── difficulty.py    # Difficulty scaling experiments
├── evaluation/          # Answer extraction and scoring
│   └── evaluator.py     # build_mcqa_prompt (SINGLE SOURCE OF TRUTH)
├── analysis/            # Analysis and visualization
│   ├── analyzer.py      # Behavioral signatures
│   └── visualize.py     # Plotting functions
├── scripts/             # CLI entry points
│   ├── process_all.py           # Process all raw datasets
│   ├── generate_distractors.py  # Generate augmented datasets
│   └── run_experiment.py        # Run evaluation experiments
├── tests/               # Test Suite
│   ├── conftest.py
│   ├── test_config.py
│   ├── test_models.py
│   ├── test_data_processing.py
│   ├── test_augmentation.py
│   └── test_integration.py
├── datasets/            # Data Directory (Managed Structure)
│   ├── raw/             # Downloaded from HuggingFace
│   ├── processed/       # Unified JSON format
│   └── augmented/       # Generated distractors
│       ├── from_scratch/
│       ├── conditioned_human/
│       └── conditioned_synthetic/
└── results/             # Experiment outputs
```

## Key Concepts

### Distractor Types (Unified Enum)
| Column | Meaning |
|--------|---------|
| `cond_human_q_a` | Human-written distractors (from original MMLU) |
| `cond_model_q_a` | Model-generated from Q+A only |
| `cond_model_q_a_dhuman` | Model-generated conditioned on human distractors |
| `cond_model_q_a_dmodel` | Model-generated conditioned on model distractors |

### Datasets
1. **MMLU-Pro** - Primary dataset, has both human and synthetic distractors
2. **ARC** - Easy and Challenge splits
3. **SuperGPQA** - Filtered to 10-option questions only

## Pipeline

1. **Download**: `python -m data.downloader --dataset all` -> `datasets/raw/`
2. **Process**: `python scripts/process_all.py` -> `datasets/processed/`
3. **Generate**: `python scripts/generate_distractors.py ...` -> `datasets/augmented/{mode}/`
4. **Filter**: `data.filter` tools
5. **Experiment**: `python scripts/run_experiment.py ...`

## Model Registry

Use `models.get_client(model_name)` to get a client instance.
Registry defined in `models/__init__.py`.

## Testing

Run the full test suite with `pytest`:
```bash
python -m pytest tests/
```

## Environment

- API keys in `.env`
- Paths: `DATASETS_DIR` (defaults to `./datasets`), `RESULTS_DIR`
- Conda env: `qgqa`
